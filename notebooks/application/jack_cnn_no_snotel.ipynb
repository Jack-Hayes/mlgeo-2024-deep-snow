{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import deep_snow.models\n",
    "from deep_snow.dataset import norm_dict\n",
    "from deep_snow.utils import calc_norm, undo_norm, calc_dowy\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = glob(\"/mnt/c/Users/JackE/uw/courses/aut24/ml_geo/jack_subsets/ncs/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "ds = xr.open_dataset(path[0])  # For example with first file\n",
    "feature_vars = ['fcf', 'elevation', 'tri', 'tpi', 'latitude', 'longitude', \n",
    "                's1_pc1', 's1_pc2', 's2_pc1', 's2_pc2', 's2_pc3', 'dowy']\n",
    "# First, reshape the PC components back to 2D (I messed up the preprocessing by flattening them, oops)\n",
    "s1_pc1_2d = ds['s1_pc1'].values.reshape(128, 128)\n",
    "s1_pc2_2d = ds['s1_pc2'].values.reshape(128, 128)\n",
    "s2_pc1_2d = ds['s2_pc1'].values.reshape(128, 128)\n",
    "s2_pc2_2d = ds['s2_pc2'].values.reshape(128, 128)\n",
    "s2_pc3_2d = ds['s2_pc3'].values.reshape(128, 128)\n",
    "# Now stack all features with consistent shapes\n",
    "features = np.stack([\n",
    "    ds['fcf'].values,\n",
    "    ds['elevation'].values,\n",
    "    ds['tri'].values,\n",
    "    ds['tpi'].values,\n",
    "    ds['latitude'].values,\n",
    "    ds['longitude'].values,\n",
    "    s1_pc1_2d,\n",
    "    s1_pc2_2d,\n",
    "    s2_pc1_2d,\n",
    "    s2_pc2_2d,\n",
    "    s2_pc3_2d,\n",
    "    ds['dowy'].values\n",
    "], axis=0) # Shape: (12, 128, 128)\n",
    "# Reshape for scaling\n",
    "original_shape = features.shape\n",
    "features_reshaped = features.reshape(len(feature_vars), -1).T  # Shape: (16384, 12)\n",
    "# Scale features\n",
    "scaler = RobustScaler()\n",
    "features_scaled = scaler.fit_transform(features_reshaped)\n",
    "# Reshape back\n",
    "features_final = features_scaled.T.reshape(original_shape)\n",
    "# Convert target (aso_sd)\n",
    "target = ds['aso_sd'].values\n",
    "# Convert to PyTorch tensors\n",
    "features_tensor = torch.FloatTensor(features_final)  # Shape: (12, 128, 128)\n",
    "target_tensor = torch.FloatTensor(target)  # Shape: (128, 128)\n",
    "# Add batch dimension if needed\n",
    "features_tensor = features_tensor.unsqueeze(0)  # Shape: (1, 12, 128, 128)\n",
    "target_tensor = target_tensor.unsqueeze(0)  # Shape: (1, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12, 128, 128),\n",
       " (12, 128, 128),\n",
       " torch.Size([1, 12, 128, 128]),\n",
       " torch.Size([1, 128, 128]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_shape, features_final.shape, features_tensor.shape, target_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "files = path[:32]\n",
    "np.random.shuffle(files)\n",
    "\n",
    "# Split files\n",
    "train_files = files[:16]  # 4 batches of 4\n",
    "test_files = files[16:24]  # 2 batches of 4\n",
    "val_files = files[24:32]  # 2 batches of 4\n",
    "\n",
    "def process_file(file_path):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    \n",
    "    # Reshape PC components to 2D\n",
    "    s1_pc1_2d = ds['s1_pc1'].values.reshape(128, 128)\n",
    "    s1_pc2_2d = ds['s1_pc2'].values.reshape(128, 128)\n",
    "    s2_pc1_2d = ds['s2_pc1'].values.reshape(128, 128)\n",
    "    s2_pc2_2d = ds['s2_pc2'].values.reshape(128, 128)\n",
    "    s2_pc3_2d = ds['s2_pc3'].values.reshape(128, 128)\n",
    "    \n",
    "    # Stack features\n",
    "    features = np.stack([\n",
    "        ds['fcf'].values,\n",
    "        ds['elevation'].values,\n",
    "        ds['tri'].values,\n",
    "        ds['tpi'].values,\n",
    "        ds['latitude'].values,\n",
    "        ds['longitude'].values,\n",
    "        s1_pc1_2d,\n",
    "        s1_pc2_2d,\n",
    "        s2_pc1_2d,\n",
    "        s2_pc2_2d,\n",
    "        s2_pc3_2d,\n",
    "        ds['dowy'].values\n",
    "    ], axis=0)\n",
    "    \n",
    "    target = ds['aso_sd'].values\n",
    "    return features, target\n",
    "\n",
    "# Create datasets\n",
    "def create_dataset(file_list):\n",
    "    features_list = []\n",
    "    targets_list = []\n",
    "    for file in file_list:\n",
    "        features, target = process_file(file)\n",
    "        features_list.append(torch.FloatTensor(features))\n",
    "        targets_list.append(torch.FloatTensor(target))\n",
    "    \n",
    "    features_tensor = torch.stack(features_list)\n",
    "    targets_tensor = torch.stack(targets_list)\n",
    "    return TensorDataset(features_tensor, targets_tensor)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = create_dataset(train_files)\n",
    "test_dataset = create_dataset(test_files)\n",
    "val_dataset = create_dataset(val_files)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch shapes:\n",
      "Features: torch.Size([4, 12, 128, 128])\n",
      "Targets: torch.Size([4, 128, 128])\n",
      "\n",
      "Test batch shapes:\n",
      "Features: torch.Size([4, 12, 128, 128])\n",
      "Targets: torch.Size([4, 128, 128])\n",
      "\n",
      "Validation batch shapes:\n",
      "Features: torch.Size([4, 12, 128, 128])\n",
      "Targets: torch.Size([4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for training loader\n",
    "for features, targets in train_loader:\n",
    "    print(\"Training batch shapes:\")\n",
    "    print(f\"Features: {features.shape}\")  # Should be (4, 12, 128, 128)\n",
    "    print(f\"Targets: {targets.shape}\")    # Should be (4, 128, 128)\n",
    "    break\n",
    "# Print shapes for test loader\n",
    "for features, targets in test_loader:\n",
    "    print(\"\\nTest batch shapes:\")\n",
    "    print(f\"Features: {features.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break\n",
    "# Print shapes for validation loader\n",
    "for features, targets in val_loader:\n",
    "    print(\"\\nValidation batch shapes:\")\n",
    "    print(f\"Features: {features.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowDepthCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SnowDepthCNN, self).__init__()\n",
    "        # First conv layer: (12, 128, 128) -> (32, 128, 128)\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second conv layer: (32, 128, 128) -> (16, 128, 128)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        # Final conv layer: (16, 128, 128) -> (1, 128, 128)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x.squeeze(1)  # Remove channel dimension to match target shape\n",
    "# Initialize model\n",
    "model = SnowDepthCNN()\n",
    "# Loss function: MAE is appropriate for continuous regression problems like snow depth\n",
    "criterion = nn.L1Loss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 4/4 [00:00<00:00, 20.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 20.1425\n",
      "Validation Loss: 25.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 4/4 [00:00<00:00, 64.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Training Loss: 13.4791\n",
      "Validation Loss: 18.3984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 4/4 [00:00<00:00, 62.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Training Loss: 10.5847\n",
      "Validation Loss: 19.0208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 4/4 [00:00<00:00, 75.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Training Loss: 9.3128\n",
      "Validation Loss: 14.2751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 4/4 [00:00<00:00, 83.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Training Loss: 9.4629\n",
      "Validation Loss: 9.8283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 4/4 [00:00<00:00, 72.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Training Loss: 4.3334\n",
      "Validation Loss: 10.9648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 4/4 [00:00<00:00, 139.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Training Loss: 5.1227\n",
      "Validation Loss: 5.3450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 4/4 [00:00<00:00, 131.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Training Loss: 2.1471\n",
      "Validation Loss: 6.8403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 4/4 [00:00<00:00, 145.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Training Loss: 3.1449\n",
      "Validation Loss: 5.1397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 4/4 [00:00<00:00, 138.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Training Loss: 2.0820\n",
      "Validation Loss: 6.5266\n",
      "\n",
      "Final Test Loss: 4.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "# Lists to store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for features, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()   \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in val_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            val_loss += criterion(outputs, targets).item()\n",
    "    # Print metrics\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "    print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "    print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "# Testing\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        features, targets = features.to(device), targets.to(device)\n",
    "        outputs = model(features)\n",
    "        test_loss += criterion(outputs, targets).item()\n",
    "print(f'\\nFinal Test Loss: {test_loss/len(test_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
