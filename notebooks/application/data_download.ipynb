{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e78065a-d4e4-42b8-a042-71a850406192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "import stackstac\n",
    "import pystac_client\n",
    "import planetary_computer\n",
    "import rasterio as rio\n",
    "import rioxarray as rxr\n",
    "from rioxarray.merge import merge_arrays\n",
    "from urllib.request import urlretrieve\n",
    "from pyproj import Proj, transform\n",
    "from os.path import basename, exists, expanduser, join\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape\n",
    "\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9bea84-1b24-4a47-ae2e-b530fb440364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/user/gbrencher/proxy/8787/status\n"
     ]
    }
   ],
   "source": [
    "client = Client()  # This starts the Dask client\n",
    "print(client.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2dac2c1-a9e1-49e4-ba1e-ac94d86900b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "        [\n",
    "            [-122.27508544921875, 47.54687159892238],\n",
    "            [-121.96128845214844, 47.54687159892238],\n",
    "            [-121.96128845214844, 47.745787772920934],\n",
    "            [-122.27508544921875, 47.745787772920934],\n",
    "            [-122.27508544921875, 47.54687159892238],\n",
    "        ]\n",
    "    ],\n",
    "}\n",
    "\n",
    "aoi_gpd = gpd.GeoDataFrame({'geometry':[shape(aoi)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69e22577-137e-4fad-afdb-181f0f83c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowon_date_range = \"2024-03-01/2024-04-01\"\n",
    "snowoff_date_range = \"2023-09-01/2023-10-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b89b4b1-ea52-47ef-ad12-d39753beb8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stac = pystac_client.Client.open(\n",
    "    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n",
    "    modifier=planetary_computer.sign_inplace,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ec7d1a-afa8-48c6-a1d2-a26e230da303",
   "metadata": {},
   "source": [
    "## grab S1 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "317b826e-c95c-42a5-8d52-ab8da9c64c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:364: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "search = stac.search(\n",
    "    intersects=aoi,\n",
    "    datetime=snowon_date_range,\n",
    "    collections=[\"sentinel-1-rtc\"],\n",
    ")\n",
    "\n",
    "items = search.item_collection()\n",
    "print(len(items))\n",
    "\n",
    "snowon_s1_ds = (\n",
    "    stackstac.stack(\n",
    "        items,\n",
    "        resolution=50\n",
    "    )\n",
    "    .where(lambda x: x > 0, other=np.nan)  # sentinel-1 uses 0 as nodata\n",
    ")\n",
    "\n",
    "# limit to morning (ascending orbit) acquisitons\n",
    "snowon_s1_ds = snowon_s1_ds.where(snowon_s1_ds.time.dt.hour > 11, drop=True)\n",
    "\n",
    "# band dimension to data variable\n",
    "snowon_s1_ds = snowon_s1_ds.to_dataset(dim='band')\n",
    "\n",
    "# clip to aoi\n",
    "snowon_s1_ds = snowon_s1_ds.rio.clip_box(*aoi_gpd.total_bounds,crs=snowon_s1_ds.rio.crs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f52afaf-fbfb-4889-a2d0-f8b809a0c54e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:364: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "search = stac.search(\n",
    "    intersects=aoi,\n",
    "    datetime=snowoff_date_range,\n",
    "    collections=[\"sentinel-1-rtc\"],\n",
    ")\n",
    "\n",
    "items = search.item_collection()\n",
    "print(len(items))\n",
    "\n",
    "snowoff_s1_ds = (\n",
    "    stackstac.stack(\n",
    "        items,\n",
    "        resolution=50\n",
    "    )\n",
    "    .where(lambda x: x > 0, other=np.nan)  # sentinel-1 uses 0 as nodata\n",
    ")\n",
    "\n",
    "# limit to morning (ascending orbit) acquisitons\n",
    "snowoff_s1_ds = snowoff_s1_ds.where(snowoff_s1_ds.time.dt.hour > 11, drop=True)\n",
    "\n",
    "# band dimension to data variable\n",
    "snowoff_s1_ds = snowoff_s1_ds.to_dataset(dim='band')\n",
    "\n",
    "# clip to aoi\n",
    "snowoff_s1_ds = snowoff_s1_ds.rio.clip_box(*aoi_gpd.total_bounds,crs=snowon_s1_ds.rio.crs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49ae5157-5c12-409c-bbe4-dbccf6220361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate medians\n",
    "# set chunks\n",
    "snowon_s1_ds = snowon_s1_ds.chunk({\"x\": 1024, \"y\": 1024, \"time\": -1})\n",
    "snowoff_s1_ds = snowoff_s1_ds.chunk({\"x\": 1024, \"y\": 1024, \"time\": -1})\n",
    "\n",
    "# compute median\n",
    "snowon_s1_ds = snowon_s1_ds.median(dim='time').squeeze()\n",
    "snowoff_s1_ds = snowoff_s1_ds.median(dim='time').squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28cc0af9-8280-4185-b55f-b55f1656b087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename variables\n",
    "snowon_s1_ds = snowon_s1_ds.rename({\n",
    "    'vv': 'snowon_vv',\n",
    "    'vh': 'snowon_vh'\n",
    "})\n",
    "\n",
    "snowoff_s1_ds = snowoff_s1_ds.rename({\n",
    "    'vv': 'snowoff_vv',\n",
    "    'vh': 'snowoff_vh'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760297a6-f40c-4733-97fd-dcfbc6be1801",
   "metadata": {},
   "source": [
    "## grab s2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "845c4341-69e8-4f4d-8ab8-b6b6b45e3b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/stackstac/prepare.py:364: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  times = pd.to_datetime(\n"
     ]
    }
   ],
   "source": [
    "search = stac.search(\n",
    "    intersects=aoi,\n",
    "    datetime=snowon_date_range,\n",
    "    collections=[\"sentinel-2-l2a\"],\n",
    "    query={\"eo:cloud_cover\": {\"lt\": 25}},\n",
    ")\n",
    "\n",
    "items = search.item_collection()\n",
    "print(len(items))\n",
    "\n",
    "s2_ds = (\n",
    "    stackstac.stack(\n",
    "        items,\n",
    "        resolution=50,\n",
    "    )\n",
    "    .where(lambda x: x > 0, other=np.nan)  # sentinel-2 uses 0 as nodata\n",
    ")\n",
    "\n",
    "# Convert the 'band' coordinate to data variables\n",
    "s2_ds = s2_ds.to_dataset(dim='band')\n",
    "\n",
    "# clip to aoi\n",
    "s2_ds = s2_ds.rio.clip_box(*aoi_gpd.total_bounds,crs=snowon_s1_ds.rio.crs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ea9f0a-d662-4f16-a89a-b9a428837f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate medians\n",
    "# set chunks\n",
    "s2_ds = s2_ds.chunk({\"x\": 1024, \"y\": 1024, \"time\": -1})\n",
    "\n",
    "# compute median\n",
    "s2_ds = s2_ds.median(dim='time').squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82eee142-4831-44bc-bcf1-097e2ad72c95",
   "metadata": {},
   "source": [
    "## grab cop30 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56490f9d-465b-4b28-b0fc-21c02c191718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "search = stac.search(\n",
    "    collections=[\"cop-dem-glo-30\"],\n",
    "    intersects=aoi\n",
    ")\n",
    "\n",
    "items = search.item_collection()\n",
    "print(len(items))\n",
    "    \n",
    "data = []\n",
    "for item in items:\n",
    "    dem_path = planetary_computer.sign(item.assets['data']).href\n",
    "    data.append(rxr.open_rasterio(dem_path))\n",
    "cop30_da = merge_arrays(data)\n",
    "cop30_ds = cop30_da.rename('elevation').squeeze().to_dataset()\n",
    "\n",
    "# clip to aoi\n",
    "cop30_ds = cop30_ds.rio.clip_box(*aoi_gpd.total_bounds,crs=snowon_s1_ds.rio.crs) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313a700-7da2-427e-a3ff-a2e3450ba0ce",
   "metadata": {},
   "source": [
    "## grab fcf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "605de344-543b-4802-b232-33179410d9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def url_download(url, out_fp, overwrite = False):\n",
    "    # check if file already exists\n",
    "    if not exists(out_fp) or overwrite == True:\n",
    "            urlretrieve(url, out_fp)\n",
    "    # if already exists. skip download.\n",
    "    else:\n",
    "        print('file already exists, skipping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e263c869-2387-4f5b-96be-749dcd5482d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_fcf(out_fp):\n",
    "    # this is the url from Lievens et al. 2021 paper\n",
    "    fcf_url = 'https://zenodo.org/record/3939050/files/PROBAV_LC100_global_v3.0.1_2019-nrt_Tree-CoverFraction-layer_EPSG-4326.tif'\n",
    "    # download just forest cover fraction to out file\n",
    "    url_download(fcf_url, out_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5169d829-8e6b-401b-820e-3bc1b01cecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcf_path ='/tmp/fcf_global.tif'\n",
    "download_fcf(fcf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4cb00e55-e6e2-4e32-b1b6-081678774ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open as dataArray and return\n",
    "fcf_ds = rxr.open_rasterio(fcf_path)\n",
    "\n",
    "# clip to aoi\n",
    "fcf_ds = fcf_ds.rio.clip_box(*aoi_gpd.total_bounds,crs=snowon_s1_ds.rio.crs) \n",
    "\n",
    "# promote to dataset\n",
    "fcf_ds = fcf_ds.rename('fcf').squeeze().to_dataset()\n",
    "\n",
    "## deal with weird values above 100!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ab0206-9524-4fa4-a873-08dc910bd468",
   "metadata": {},
   "source": [
    "## combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbf541-46a7-4299-abb8-6757f2b65a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproject to match snowon s1\n",
    "snowoff_s1_ds = snowoff_s1_ds.rio.reproject_match(snowon_s1_ds)\n",
    "s2_ds = s2_ds.rio.reproject_match(snowon_s1_ds)\n",
    "cop30_da = cop30_da.rio.reproject_match(snowon_s1_ds)\n",
    "fcf_ds = fcf_ds.rio.reproject_match(snowon_s1_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1bf69e-f466-4b00-8d75-4978527019b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = [snowon_s1_ds, snowoff_s1_ds, s2_ds, cop30_da, fcf_ds]\n",
    "ds = xr.merge(ds_list, compat='override', join='override').squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae057f96-adc8-4813-88d9-78579fa81862",
   "metadata": {},
   "source": [
    "## calculate additional data variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31608a02-215c-4087-9502-bafc6a9a72a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cross ratios\n",
    "def db_scale(x, epsilon=1e-10):\n",
    "    # Add epsilon only where x is zero\n",
    "    x_with_epsilon = np.where(x==0, epsilon, x)\n",
    "    # Calculate the logarithm\n",
    "    log_x = 10 * np.log10(x_with_epsilon)\n",
    "    # Set the areas where x was originally zero back to zero\n",
    "    log_x[x==0] = 0\n",
    "    return log_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "97b5f0cf-e228-4833-a621-e21312b17293",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'db_scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# radar data variables\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowon_vv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdb_scale\u001b[49m(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowon_vv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      3\u001b[0m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowon_vv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m db_scale(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowon_vv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowon_vv\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m db_scale(ds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnowon_vv\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'db_scale' is not defined"
     ]
    }
   ],
   "source": [
    "# radar data variables\n",
    "# convert to decibels\n",
    "ds['snowon_vv'] = db_scale(ds['snowon_vv'])\n",
    "ds['snowon_vh'] = db_scale(ds['snowon_vh'])\n",
    "ds['snowoff_vv'] = db_scale(ds['snowoff_vv'])\n",
    "ds['snowoff_vh'] = db_scale(ds['snowoff_vh'])\n",
    "\n",
    "# calculate variables\n",
    "ds['snowon_cr'] = ds['snowon_vh'] - ds['snowon_vv']\n",
    "ds['snowoff_cr'] = ds['snowoff_vh'] - ds['snowoff_vv']\n",
    "ds['delta_cr'] = ds['snowon_cr'] - ds['snowoff_cr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20377431-6710-47f6-a770-d7044f3c5d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s2 band indices\n",
    "ds['ndvi'] = (ds['B08'] - ds['B04'])/(ds['B08'] + ds['B04'])\n",
    "ds['ndsi'] = (ds['B03'] - ds['B11'])/(ds['B03'] + ds['B11'])\n",
    "ds['ndwi'] = (ds['B03'] - ds['B08'])/(ds['B03'] + ds['B08'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1cc5fe-2dd0-41b5-8eed-ee7c007affa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude, longitude\n",
    "# define projections\n",
    "utm_proj = Proj(proj='utm', zone='10', ellps='WGS84') ## NOTE hardcoded utm for now, adjust before use\n",
    "wgs84_proj = Proj(proj='latlong', datum='WGS84')\n",
    "\n",
    "x, y = np.meshgrid(ds['x'].values, ds['y'].values)\n",
    "lon, lat = transform(utm_proj, wgs84_proj, x, y)\n",
    "ds['latitude'] = (('y', 'x'), lat)\n",
    "ds['longitude'] = (('y', 'x'), lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccf312-0535-4e80-b515-efea997d1385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dowy\n",
    "def calc_dowy(doy):\n",
    "    'calculate day of water year from day of year'\n",
    "    if doy < 274:\n",
    "        dowy = doy + (365-274)\n",
    "    elif doy >= 274:\n",
    "        dowy = doy-274\n",
    "    return dowy\n",
    "\n",
    "## NOTE think about date and fix this\n",
    "dowy_1d = calc_dowy(pd.to_datetime(fn.split('_')[4]).dayofyear)\n",
    "dowy = torch.full_like(aso_sd, dowy_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816be9a-400e-4320-882a-aa790b7f2b83",
   "metadata": {},
   "source": [
    "## write out to zarr file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0bb0f-a1bc-4137-bba1-40cfecd4fb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_zarr(f'{data_path}/combined/stack_20230725_20230925.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12ef199-8399-460d-a257-e7d32e9e3685",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
