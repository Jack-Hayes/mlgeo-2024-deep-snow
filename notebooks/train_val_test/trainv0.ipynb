{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "564e05be-b59d-4241-9bb9-fda9830daca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "import xarray as xr\n",
    "from torch.masked import masked_tensor, as_masked_tensor\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b86af2f3-e94c-492f-a171-1d8ad61d88f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = '/home/jovyan/shared-public/crunchy-snow/data/subsetsv1/train'\n",
    "train_path_list = glob(f'{train_data_dir}/ASO_50M_SD*.nc')[:20]\n",
    "\n",
    "val_data_dir = '/home/jovyan/shared-public/crunchy-snow/data/subsetsv1/val'\n",
    "val_path_list = glob(f'{val_data_dir}/ASO_50M_SD*.nc')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "409892b2-4bda-4082-86ee-488c50a9202e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are set by finding the min and max across the entire dataset\n",
    "norm_dict = {'aso_sd':[0, 24.9],\n",
    "             'vv':[0, 13523.8],\n",
    "             'vh':[0, 43.2],\n",
    "             'AOT':[0, 572.1],\n",
    "             'coastal':[0, 23459.1],\n",
    "             'blue':[0, 23004.1],\n",
    "             'green':[0, 26440.1],\n",
    "             'red':[0, 21576.1],\n",
    "             'red_edge1':[0, 20796.1],\n",
    "             'red_edge2':[0, 20432.1],\n",
    "             'red_edge3':[0, 20149.1],\n",
    "             'nir':[0, 21217.1],\n",
    "             'water_vapor':[0, 18199.1],\n",
    "             'swir1':[0, 17549.1],\n",
    "             'swir2':[0, 17314.1],\n",
    "             'scene_class_map':[0, 15],\n",
    "             'water_vapor_product':[0, 6517.5],\n",
    "             'elevation':[-100, 9000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c864618c-d625-4c99-b4bd-fb5256e394fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_norm(tensor, minmax_list):\n",
    "    '''\n",
    "    normalize a tensor between 0 and 1 using a min and max value stored in a list\n",
    "    '''\n",
    "    normalized = (tensor-minmax_list[0])/(minmax_list[1]-minmax_list[0])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48286bb5-c971-498a-9258-2664d6d0bd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dataset \n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    class that reads data from a netCDF and returns normalized tensors \n",
    "    '''\n",
    "    def __init__(self, path_list, norm_dict, norm=True):\n",
    "        self.path_list = path_list\n",
    "        self.norm_dict = norm_dict\n",
    "        self.norm = norm\n",
    "        \n",
    "    #dataset length\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.path_list)\n",
    "        return self.filelength\n",
    "    \n",
    "    #load images\n",
    "    def __getitem__(self,idx):\n",
    "        ds = xr.open_dataset(self.path_list[idx])\n",
    "        #ds = ds.coarsen(x = 6, boundary = 'trim').mean().coarsen(y = 6, boundary = 'trim').mean()\n",
    "        # convert to tensors\n",
    "        aso_sd = torch.from_numpy(np.float32(ds.aso_sd.values))\n",
    "        snowon_vv = torch.from_numpy(np.float32(ds.snowon_vv.values))\n",
    "        snowon_vh = torch.from_numpy(np.float32(ds.snowon_vh.values))\n",
    "        snowoff_vv = torch.from_numpy(np.float32(ds.snowoff_vv.values))\n",
    "        snowoff_vh = torch.from_numpy(np.float32(ds.snowoff_vh.values))\n",
    "        snowon_vv_mean = torch.from_numpy(np.float32(ds.snowon_vv_mean.values))\n",
    "        snowon_vh_mean = torch.from_numpy(np.float32(ds.snowon_vh_mean.values))\n",
    "        snowoff_vv_mean = torch.from_numpy(np.float32(ds.snowoff_vv_mean.values))\n",
    "        snowoff_vh_mean = torch.from_numpy(np.float32(ds.snowoff_vh_mean.values))\n",
    "        aerosol_optical_thickness = torch.from_numpy(np.float32(ds.AOT.values))\n",
    "        coastal_aerosol = torch.from_numpy(np.float32(ds.B01.values))\n",
    "        blue = torch.from_numpy(np.float32(ds.B02.values))\n",
    "        green = torch.from_numpy(np.float32(ds.B03.values))\n",
    "        red = torch.from_numpy(np.float32(ds.B04.values))\n",
    "        red_edge1 = torch.from_numpy(np.float32(ds.B05.values))\n",
    "        red_edge2 = torch.from_numpy(np.float32(ds.B06.values))\n",
    "        red_edge3 = torch.from_numpy(np.float32(ds.B07.values))\n",
    "        nir = torch.from_numpy(np.float32(ds.B08.values))\n",
    "        water_vapor = torch.from_numpy(np.float32(ds.B09.values))\n",
    "        swir1 = torch.from_numpy(np.float32(ds.B11.values))\n",
    "        swir2 = torch.from_numpy(np.float32(ds.B12.values))\n",
    "        scene_class_map = torch.from_numpy(np.float32(ds.SCL.values))\n",
    "        water_vapor_product = torch.from_numpy(np.float32(ds.WVP.values))\n",
    "        fcf = torch.from_numpy(np.float32(ds.fcf.values))\n",
    "        elevation = torch.from_numpy(np.float32(ds.elevation.values))\n",
    "        aso_gap_map = torch.from_numpy(np.float32(ds.aso_gap_map.values))\n",
    "        rtc_gap_map = torch.from_numpy(np.float32(ds.rtc_gap_map.values))\n",
    "        rtc_mean_gap_map = torch.from_numpy(np.float32(ds.rtc_mean_gap_map.values))\n",
    "            \n",
    "        # normalize layers (except gap maps and fcf)\n",
    "        if self.norm == True:\n",
    "            aso_sd = calc_norm(aso_sd, self.norm_dict['aso_sd'])\n",
    "            snowon_vv = calc_norm(snowon_vv, self.norm_dict['vv'])\n",
    "            snowon_vh = calc_norm(snowon_vh, self.norm_dict['vh'])\n",
    "            snowoff_vv = calc_norm(snowoff_vv, self.norm_dict['vv'])\n",
    "            snowoff_vh = calc_norm(snowoff_vh, self.norm_dict['vh'])\n",
    "            snowon_vv_mean = calc_norm(snowon_vv_mean, self.norm_dict['vv'])\n",
    "            snowon_vh_mean = calc_norm(snowon_vh_mean, self.norm_dict['vh'])\n",
    "            snowoff_vv_mean = calc_norm(snowoff_vv_mean, self.norm_dict['vv'])\n",
    "            snowoff_vh_mean = calc_norm(snowoff_vh_mean, self.norm_dict['vh'])\n",
    "            aerosol_optical_thickness = calc_norm(aerosol_optical_thickness, self.norm_dict['AOT'])\n",
    "            coastal_aerosol = calc_norm(coastal_aerosol, self.norm_dict['coastal'])\n",
    "            blue = calc_norm(blue, self.norm_dict['blue'])\n",
    "            green = calc_norm(green, self.norm_dict['green'])\n",
    "            red = calc_norm(red, self.norm_dict['red'])\n",
    "            red_edge1 = calc_norm(red_edge1, self.norm_dict['red_edge1'])\n",
    "            red_edge2 = calc_norm(red_edge2, self.norm_dict['red_edge2'])\n",
    "            red_edge3 = calc_norm(red_edge3, self.norm_dict['red_edge3'])\n",
    "            nir = calc_norm(nir, self.norm_dict['nir'])\n",
    "            water_vapor = calc_norm(water_vapor, self.norm_dict['water_vapor'])\n",
    "            swir1 = calc_norm(swir1, self.norm_dict['swir1'])\n",
    "            swir2 = calc_norm(swir2, self.norm_dict['swir2'])\n",
    "            scene_class_map = calc_norm(scene_class_map, self.norm_dict['scene_class_map'])\n",
    "            water_vapor_product = calc_norm(water_vapor_product, self.norm_dict['water_vapor_product'])\n",
    "            elevation = calc_norm(elevation, self.norm_dict['elevation'])\n",
    "         \n",
    "        # return only selected bands, for now\n",
    "        return aso_sd[None, :, :], snowon_vv[None, :, :], snowon_vh[None, :, :], snowoff_vv[None, :, :], snowoff_vh[None, :, :], blue[None, :, :], green[None, :, :], red[None, :, :], fcf[None, :, :], elevation[None, :, :], aso_gap_map[None, :, :], rtc_gap_map[None, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb092ae1-84a9-4edf-8473-5e49c94a4c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "# create dataloaders\n",
    "train_data = dataset(train_path_list, norm_dict, norm=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n",
    "val_data = dataset(val_path_list, norm_dict, norm=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "217fc025-54ee-4097-a75b-84b2656b8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # note--this will break sometimes. The underlying data is fine but we haven't spent any time refining this plotting code\n",
    "# sns.set_theme()\n",
    "# # plot normalized training inputs \n",
    "# num_samples = 1\n",
    "\n",
    "# for i, (aso_sd, snowon_vv, snowon_vh, snowoff_vv, snowoff_vh, blue, green, red, nir, swir1, swir2, fcf, elevation, aso_gap_map, rtc_gap_map) in enumerate(train_loader):\n",
    "#     if i < num_samples:\n",
    "#             f, ax = plt.subplots(3, 4, figsize=(10,8), sharey=True, sharex=True)\n",
    "#             ax[0, 0].imshow(aso_sd.squeeze(), cmap='Blues', vmin=0, vmax=0.2, interpolation='none') \n",
    "#             ax[0, 0].set_title('ASO snow depths')\n",
    "#             ax[0, 1].imshow(elevation.squeeze(), cmap='viridis', interpolation='none') \n",
    "#             ax[0, 1].set_title('Copernicus DEM')\n",
    "#             ax[0, 2].imshow(fcf.squeeze(), cmap='Greens', interpolation='none') \n",
    "#             ax[0, 2].set_title('fractional forest cover')\n",
    "#             norm_max = np.max([green.max(), red.max(), blue.max()]) # there are better ways to do this\n",
    "#             ax[0, 3].imshow(torch.cat((red[:, :, :, None]/norm_max, green[:, :, :, None]/norm_max, blue[:, :, :, None]/norm_max), 3).squeeze(), interpolation='none') \n",
    "#             ax[0, 3].set_title('true color image')\n",
    "#             ax[1, 0].imshow(np.log(snowon_vv.squeeze()), cmap='Greys_r', interpolation='none') \n",
    "#             ax[1, 0].set_title('S1 snow-on vv')\n",
    "#             ax[1, 1].imshow(np.log(snowon_vh.squeeze()), cmap='Greys_r', interpolation='none') \n",
    "#             ax[1, 1].set_title('S1 snow-on vh')\n",
    "#             ax[1, 2].imshow(np.log(snowoff_vv.squeeze()), cmap='Greys_r', interpolation='none') \n",
    "#             ax[1, 2].set_title('S1 snow-off vv')\n",
    "#             ax[1, 3].imshow(np.log(snowoff_vh.squeeze()), cmap='Greys_r', interpolation='none') \n",
    "#             ax[1, 3].set_title('S1 snow-off vh')\n",
    "#             ax[2, 0].imshow(snowon_vh.squeeze()/snowon_vv.squeeze(), cmap='Purples', interpolation='none') \n",
    "#             ax[2, 0].set_title('snow-on vh/vv')\n",
    "#             ax[2, 1].imshow(snowoff_vh.squeeze()/snowoff_vv.squeeze(), cmap='Purples', interpolation='none') \n",
    "#             ax[2, 1].set_title('snow-off vh/vv')\n",
    "#             ax[2, 2].imshow((snowon_vh.squeeze()/snowon_vv.squeeze()-snowoff_vh.squeeze()/snowoff_vv.squeeze())+0.1, cmap='RdBu', interpolation='none') \n",
    "#             ax[2, 2].set_title('snow-on vh/vv - snow-off vh/vv')\n",
    "#             ax[2, 3].imshow(aso_gap_map.squeeze()+rtc_gap_map.squeeze(), cmap='Purples', interpolation='none') \n",
    "#             ax[2, 3].set_title('aso and rtc gaps')\n",
    "#             ax[0, 0].set_aspect('equal')\n",
    "            \n",
    "#             f.tight_layout()\n",
    "#     else: \n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cbae9e2-0e4d-477b-9793-68173d10cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', in_channels=9, out_channels=1, init_features=32, pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5110eab-738c-4206-9333-dc12a6909e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_channels, out_channels, stride=1, padding=1, bias=True):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=bias)\n",
    "\n",
    "\n",
    "def conv1x1(in_channels, out_channels):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1)\n",
    "\n",
    "\n",
    "def check_valid_activation(choice):\n",
    "    if choice not in ['relu', 'lrelu', 'prelu']:\n",
    "        raise ValueError(f\"'{choice}' is not a valid activation function. Choose among ['relu', 'lrelu', 'prelu'].\\n\")\n",
    "\n",
    "\n",
    "def upconv(in_channels, out_channels, mode='transpose'):\n",
    "    # stride=2 implies upsampling by a factor of 2\n",
    "    get_up_mode = nn.ModuleDict([\n",
    "        ['bilinear', nn.Sequential(nn.Upsample(mode='bilinear', scale_factor=2), conv1x1(in_channels, out_channels))],\n",
    "        ['transpose', nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)]\n",
    "    ])\n",
    "\n",
    "    return get_up_mode[mode]\n",
    "\n",
    "\n",
    "def get_activation(choice):\n",
    "    activation_functions = nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['lrelu', nn.LeakyReLU(inplace=True)],\n",
    "        ['prelu', nn.PReLU()]\n",
    "        ])\n",
    "    return activation_functions[choice]\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Partial encoder block consisting of a 3×3 convolutional layer with stride 1, followed by batch normalization\n",
    "    (optional) and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "def conv_up_block(in_channels, out_channels, activation='relu', do_BN=True, up_mode='transpose', *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Decoder block consisting of an up-convolutional layer, followed by a 3×3 convolutional layer with stride 1,\n",
    "    batch normalization (optional), and a non-linear activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            upconv(in_channels, in_channels, up_mode),\n",
    "            nn.Sequential(\n",
    "                conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "                get_activation(activation))\n",
    "            )\n",
    "\n",
    "\n",
    "def bottleneck(in_channels, out_channels, activation='relu', do_BN=True, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Bottleneck block.\n",
    "    \"\"\"\n",
    "\n",
    "    if do_BN:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=False, *args, **kwargs),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            conv3x3(in_channels, out_channels, bias=True, *args, **kwargs),\n",
    "            get_activation(activation)\n",
    "        )\n",
    "\n",
    "\n",
    "class SkipConnection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SkipConnection, self).__init__()\n",
    "\n",
    "    def forward(self, x_skip, x_up):\n",
    "        return x_skip + x_up\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_input_channels=9, start_kernel=64, max_filter_depth=512, depth=5,\n",
    "                 act_fn_encoder='relu', act_fn_decoder='relu', act_fn_bottleneck='relu', up_mode='transpose',\n",
    "                 do_BN=False, bias_conv_layer=True, outer_skip=True, outer_skip_BN=False):\n",
    "        \"\"\"\n",
    "        UNet network architecture.\n",
    "        :param n_input_channels:    int, number of input channels\n",
    "        :param start_kernel:        int, number of filters of the first convolutional layer in the encoder\n",
    "        :param max_filter_depth:    int, maximum filter depth\n",
    "        :param depth:               int, number of downsampling and upsampling layers (i.e., number of blocks in the\n",
    "                                    encoder and decoder)\n",
    "        :param act_fn_encoder:      str, activation function used in the encoder\n",
    "        :param act_fn_decoder:      str, activation function used in the decoder\n",
    "        :param act_fn_bottleneck:   str, activation function used in the bottleneck\n",
    "        :param up_mode:             str, upsampling mode\n",
    "        :param do_BN:               boolean, True to perform batch normalization after every convolutional layer,\n",
    "                                    False otherwise\n",
    "        :param bias_conv_layer:     boolean, True to activate the learnable bias of the convolutional layers,\n",
    "                                    False otherwise\n",
    "        :param outer_skip:          boolean, True to activate the long residual skip connection that adds the\n",
    "                                    initial DSM to the output of the last decoder layer, False otherwise\n",
    "        :param outer_skip_BN:       boolean, True to add batch normalization to the long residual skip connection,\n",
    "                                    False otherwise\n",
    "        \"\"\"\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        check_valid_activation(act_fn_encoder)\n",
    "        check_valid_activation(act_fn_decoder)\n",
    "        check_valid_activation(act_fn_bottleneck)\n",
    "\n",
    "        if up_mode not in ['transpose', 'bilinear']:\n",
    "            raise ValueError(f\"'{up_mode}' is not a valid mode for upsampling. Choose among ['transpose', 'bilinear'] \"\n",
    "                             \"to specify 'up_mode'.\\n\")\n",
    "\n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.start_kernel = start_kernel\n",
    "        self.depth = depth\n",
    "        self.act_fn_encoder = act_fn_encoder\n",
    "        self.act_fn_decoder = act_fn_decoder\n",
    "        self.act_fn_bottleneck = act_fn_bottleneck\n",
    "        self.up_mode = up_mode\n",
    "        self.max_filter_depth = max_filter_depth\n",
    "        self.do_BN = do_BN\n",
    "        self.bias_conv_layer = bias_conv_layer\n",
    "        self.do_outer_skip = outer_skip\n",
    "        self.do_outer_skip_BN = outer_skip_BN\n",
    "        self.filter_depths = [self.start_kernel * (2 ** i) for i in range(self.depth)]\n",
    "\n",
    "        # Restrict the maximum filter depth to a predefined value\n",
    "        self.filter_depths = [self.max_filter_depth if i > self.max_filter_depth else i for i in self.filter_depths]\n",
    "\n",
    "        # Set up the encoder\n",
    "        self.encoder = nn.ModuleList()\n",
    "        self.encoder.append(nn.Sequential(\n",
    "            conv_block(self.n_input_channels, self.start_kernel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths, self.filter_depths[1:]):\n",
    "            self.encoder.append(nn.Sequential(\n",
    "                conv_block(in_channel, out_channel, activation=self.act_fn_encoder, do_BN=self.do_BN),\n",
    "                nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            ))\n",
    "\n",
    "        # Set up the bottleneck\n",
    "        self.bottleneck = bottleneck(self.filter_depths[-1], self.filter_depths[-1], activation=self.act_fn_bottleneck,\n",
    "                                     do_BN=self.do_BN)\n",
    "\n",
    "        # Set up the decoder\n",
    "        self.decoder = nn.ModuleList()\n",
    "        self.filter_depths_up = list(reversed(self.filter_depths))\n",
    "\n",
    "        for in_channel, out_channel in zip(self.filter_depths_up[:-1], self.filter_depths_up[1:]):\n",
    "            self.decoder.append(conv_up_block(in_channel, out_channel, activation=self.act_fn_decoder,\n",
    "                                              up_mode=self.up_mode, do_BN=self.do_BN))\n",
    "        self.decoder.append(upconv(self.filter_depths_up[-1], self.filter_depths_up[-1], up_mode))\n",
    "\n",
    "        # Set up the final layer of the decoder\n",
    "        self.last_layer = conv3x3(self.start_kernel, 1, bias=self.bias_conv_layer)\n",
    "\n",
    "        # Skip connection\n",
    "        self.skipconnect = SkipConnection()\n",
    "\n",
    "        # Batch normalization added to the long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            self.layer_outer_skip = nn.ModuleList()\n",
    "            if self.do_outer_skip_BN:\n",
    "                self.layer_outer_skip.append(nn.BatchNorm2d(1))\n",
    "            self.layer_outer_skip.append(SkipConnection())\n",
    "\n",
    "    def forward(self, snowon_vv, snowon_vh, snowoff_vv, snowoff_vh, blue, green, red, fcf, elevation):\n",
    "        skip_connections = []\n",
    "        x = torch.cat((snowon_vv, snowon_vh, snowoff_vv, snowoff_vh, blue, green, red, fcf, elevation), dim=1)\n",
    "        out = x\n",
    "\n",
    "        # Encoder (save intermediate outputs for skip connections)\n",
    "        for index, layer in enumerate(self.encoder):\n",
    "            layer_conv = layer[:-1]  # all layers before the pooling layer (at depth index)\n",
    "            layer_pool = layer[-1]   # pooling layer (at depth index)\n",
    "\n",
    "            out_before_pool = layer_conv(out)\n",
    "            skip_connections.append(out_before_pool)\n",
    "            out = layer_pool(out_before_pool)\n",
    "\n",
    "        # Bottleneck\n",
    "        out = self.bottleneck(out)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        index_max = len(self.decoder) - 1\n",
    "        for index, layer in enumerate(self.decoder):\n",
    "            if index <= index_max - 1:\n",
    "                layer_upconv = layer[0]  # upconv layer\n",
    "                layer_conv = layer[1::]  # all other layers (conv, batchnorm, activation)\n",
    "\n",
    "                out_temp = layer_upconv(out)\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "                out = layer_conv(out)\n",
    "            else:\n",
    "                out_temp = layer(out)   # upconv of last layer\n",
    "                out = self.skipconnect(skip_connections[-1 - index], out_temp)\n",
    "\n",
    "        # Last layer of the decoder\n",
    "        out = self.last_layer(out)\n",
    "\n",
    "        # Add long residual skip connection\n",
    "        if self.do_outer_skip:\n",
    "            if self.layer_outer_skip.__len__() == 2:\n",
    "                # pipe input through a batch normalization layer before adding it to the output of the last\n",
    "                # decoder layer\n",
    "                bn = self.layer_outer_skip[0]\n",
    "                x_0 = x[:, 0, :, :]       # use channel 0 only\n",
    "                x_0 = x_0.unsqueeze(1)\n",
    "                x = bn(x_0)\n",
    "\n",
    "            # add (batchnorm) input to the output of the last decoder layer\n",
    "            add = self.layer_outer_skip[-1]\n",
    "            x_0 = x[:, 0, :, :]\n",
    "            x_0 = x_0.unsqueeze(1)\n",
    "\n",
    "            out = add(x_0, out)  # use channel 0 only\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1aed2a4-d585-42ce-b9d9-0ebb099dcb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83d2d8c5-97a2-449b-b38e-a18858023c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "starting epoch 0\n",
      "training loss: 0.027002938091754913\n",
      "validation loss: 0.025665277597727253\n",
      "\n",
      "starting epoch 1\n",
      "training loss: 0.02509717456996441\n",
      "validation loss: 0.025665277597727253\n",
      "\n",
      "starting epoch 2\n",
      "training loss: 0.04642945993691683\n",
      "validation loss: 0.025665277597727253\n",
      "\n",
      "starting epoch 3\n",
      "training loss: 0.033049569465219975\n",
      "validation loss: 0.025665277597727253\n",
      "CPU times: user 8.07 s, sys: 1.66 s, total: 9.72 s\n",
      "Wall time: 15.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Define optimizer\n",
    "model.to('cuda') # run on gpu\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0003)\n",
    "loss_fn   = nn.L1Loss()\n",
    "epochs = 4\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\nstarting epoch {epoch}')\n",
    "    epoch_loss = []\n",
    "    val_temp_loss = []\n",
    "    \n",
    "    #loop through training data \n",
    "    for (aso_sd, snowon_vv, snowon_vh, snowoff_vv, snowoff_vh, blue, green, red, fcf, elevation, aso_gap_map, rtc_gap_map) in train_loader:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        pred_sd = torch.clamp(model(snowon_vv.to('cuda'), snowon_vh.to('cuda'), snowoff_vv.to('cuda'), snowoff_vh.to('cuda'), blue.to('cuda'), green.to('cuda'), red.to('cuda'), fcf.to('cuda'), elevation.to('cuda')), 0, 1) # Generate noise predictions\n",
    "    \n",
    "        # calculate predicted signals \n",
    "        loss = loss_fn(pred_sd.to('cuda'), aso_sd.to('cuda')) # calculate loss \n",
    "        epoch_loss.append(loss.item()) # add batch loss to epoch loss list\n",
    "        \n",
    "        loss.backward() #Propagate the gradients in backward pass\n",
    "        optimizer.step() \n",
    "\n",
    "    train_loss.append(np.mean(epoch_loss))\n",
    "    print(f'training loss: {np.mean(epoch_loss)}')\n",
    "    \n",
    "    # run model on validation data \n",
    "    for (aso_sd, snowon_vv, snowon_vh, snowoff_vv, snowoff_vh, blue, green, red, fcf, elevation, aso_gap_map, rtc_gap_map) in val_loader:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            \n",
    "            pred_sd = torch.clamp(model(snowon_vv.to('cuda'), snowon_vh.to('cuda'), snowoff_vv.to('cuda'), snowoff_vh.to('cuda'), blue.to('cuda'), green.to('cuda'), red.to('cuda'), fcf.to('cuda'), elevation.to('cuda')), 0, 1) #Generate predictions using the model\n",
    "           \n",
    "            loss = loss_fn(pred_sd.to('cuda'), aso_sd.to('cuda')) #Loss/error\n",
    "            val_temp_loss.append(loss.item())\n",
    "    \n",
    "    val_loss.append(np.mean(val_temp_loss))\n",
    "    print(f'validation loss: {np.mean(val_temp_loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3f0fa7f-7ed4-45c4-acac-18b56d78739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize output\n",
    "output = model(snowon_vv.to('cuda'), snowon_vh.to('cuda'), snowoff_vv.to('cuda'), snowoff_vh.to('cuda'), blue.to('cuda'), green.to('cuda'), red.to('cuda'), fcf.to('cuda'), elevation.to('cuda'))\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c886438a-c05e-41b4-b903-246d4d35d723",
   "metadata": {},
   "outputs": [],
   "source": [
    " with torch.no_grad():\n",
    "            pred_sd = model(snowon_vv.to('cuda'), snowon_vh.to('cuda'), snowoff_vv.to('cuda'), snowoff_vh.to('cuda'), blue.to('cuda'), green.to('cuda'), red.to('cuda'), fcf.to('cuda'), elevation.to('cuda')) #Generate predictions using the model\n",
    "            pred_sd = pred_sd.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "774a5cbe-858d-4296-ada8-963caf133d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(pred_sd))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
