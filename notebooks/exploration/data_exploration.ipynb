{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1636ee08-d91b-447a-a32a-d3e2c274e829",
   "metadata": {},
   "source": [
    "# Explore training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6b4bd3a-59e0-438b-90b8-ac0c67871523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import crunchy_snow.models\n",
    "import crunchy_snow.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe58eeaa-abdb-47d0-9c78-0d338adc8fad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/mnt/Backups/gbrench/repos/crunchy-snow/data/subsetsv1/train'\n",
    "path_list = glob(f'{data_dir}/ASO_50M_SD*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bad5b6b-809d-4b81-a229-1a8042507591",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_channels = ['snowon_cr', # cross ratio, snowon_vh - snowon_vv\n",
    "                     'snowoff_cr', # cross ratio, snowoff_vh - snowoff_vv\n",
    "                     'delta_cr' # change in cross ratio, snowon_cr - snowoff_cr\n",
    "                    ]\n",
    "\n",
    "# prepare training and validation dataloaders\n",
    "train_data = crunchy_snow.dataset.Dataset(path_list, selected_channels, norm=False)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=2048, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d92cab-7886-42fa-9e33-95c7bd21f54f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop 1\n",
      "loop 2\n",
      "loop 3\n"
     ]
    }
   ],
   "source": [
    "# find dataset min and max for normalization\n",
    "norm_dict = {}\n",
    "for i, outputs in enumerate(train_loader):\n",
    "    print(f'loop {i+1}')\n",
    "    for j, item in enumerate(outputs):\n",
    "        if i == 0:\n",
    "            norm_dict[j] = [item.min(), item.max()]\n",
    "        if item.max() > norm_dict[j][1]:\n",
    "            norm_dict[j][1] = item.max().item()\n",
    "        if item.min() < norm_dict[j][0] and not item.min() == 0:\n",
    "            norm_dict[j][0] = item.min().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27671fe8-1541-487c-9222-d33978667e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(norm_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bfe91-01eb-4449-83d9-eec863a3c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data to be returned by dataloader\n",
    "selected_channels = [\n",
    "    # ASO products\n",
    "    'aso_sd', # ASO lidar snow depth (target dataset)\n",
    "    'aso_gap_map', # gaps in ASO data\n",
    "    \n",
    "    # Sentinel-1 products\n",
    "    'snowon_vv', # snow on Sentinel-1 VV polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowon_vh', # snow on Sentinel-1 VH polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowoff_vv', # snow off Sentinel-1 VV polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowoff_vh', # snow off Sentinel-1 VH polarization backscatter in dB, closest acquisition to ASO acquisition\n",
    "    'snowon_vv_mean', # snow on Sentinel-1 VV polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowon_vh_mean', # snow on Sentinel-1 VH polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowoff_vv_mean', # snow off Sentinel-1 VV polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowoff_vh_mean', # snow off Sentinel-1 VH polarization backscatter in dB, mean of acquisition in 4 week period around ASO acquisition\n",
    "    'snowon_cr', # cross ratio, snowon_vh - snowon_vv\n",
    "    'snowoff_cr', # cross ratio, snowoff_vh - snowoff_vv\n",
    "    'delta_cr', # change in cross ratio, snowon_cr - snowoff_cr\n",
    "    'rtc_gap_map', # gaps in Sentinel-1 data\n",
    "    'rtc_mean_gap_map', # gaps in Sentinel-1 mean data\n",
    "    \n",
    "    # Sentinel-2 products \n",
    "    'aerosol_optical_thickness', # snow on Sentinel-2 aerosol optical thickness band \n",
    "    'coastal_aerosol', # snow on Sentinel-2 coastal aerosol band\n",
    "    'blue', # snow on Sentinel-2 blue band\n",
    "    'green', # snow on Sentinel-2 green band\n",
    "    'red', # snow on Sentinel-2 red band\n",
    "    'red_edge1', # snow on Sentinel-2 red edge 1 band\n",
    "    'red_edge2', # snow on Sentinel-2 red edge 2 band\n",
    "    'red_edge3', # snow on Sentinel-2 red edge 3 band\n",
    "    'nir', # snow on Sentinel-2 near infrared band\n",
    "    'water_vapor', # snow on Sentinel-2 water vapor\n",
    "    'swir1', # snow on Sentinel-2 shortwave infrared band 1\n",
    "    'swir2', # snow on Sentinel-2 shortwave infrared band 2\n",
    "    'scene_class_map', # snow on Sentinel-2 scene classification product\n",
    "    'water_vapor_product', # snow on Sentinel-2 water vapor product\n",
    "    'ndvi', # Normalized Difference Vegetation Index from Sentinel-2\n",
    "    'ndsi', # Normalized Difference Snow Index from Sentinel-2\n",
    "    'ndwi', # Normalized Difference Water Index from Sentinel-2\n",
    "\n",
    "    # PROBA-V global land cover dataset (Buchhorn et al., 2020)\n",
    "    'fcf', # fractional forest cover\n",
    "    \n",
    "    # COP30 digital elevation model      \n",
    "    'elevation' # elevation\n",
    "                    ]\n",
    "# prepare training and validation dataloaders\n",
    "train_data = crunchy_snow.dataset.Dataset(path_list, selected_channels, norm=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdf5fb2-f31c-481d-ab60-ea10c3cc641f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot example normalized training images\n",
    "sns.set_theme()\n",
    "num_samples = 1\n",
    "\n",
    "for i, data_tuple in enumerate(train_loader):\n",
    "    if i < num_samples:\n",
    "            data_dict = {name: tensor for name, tensor in zip(selected_channels, data_tuple)}\n",
    "            f, ax = plt.subplots(3, 4, figsize=(10,8), sharey=True, sharex=True)\n",
    "            ax[0, 0].imshow(data_dict['aso_sd'].squeeze(), cmap='Blues', vmin=0, vmax=0.2, interpolation='none') \n",
    "            ax[0, 0].set_title('ASO snow depths')\n",
    "            ax[0, 1].imshow(data_dict['elevation'].squeeze(), cmap='viridis', interpolation='none') \n",
    "            ax[0, 1].set_title('Copernicus DEM')\n",
    "            ax[0, 2].imshow(data_dict['fcf'].squeeze(), cmap='Greens', interpolation='none') \n",
    "            ax[0, 2].set_title('fractional forest cover')\n",
    "            norm_max = np.max([data_dict['green'].max(), data_dict['red'].max(), data_dict['blue'].max()]) # there are better ways to do this\n",
    "            ax[0, 3].imshow(torch.cat((data_dict['red'][:, :, :, None]/norm_max, data_dict['green'][:, :, :, None]/norm_max, data_dict['blue'][:, :, :, None]/norm_max), 3).squeeze(), interpolation='none') \n",
    "            ax[0, 3].set_title('true color image')\n",
    "            ax[1, 0].imshow(np.log(data_dict['snowon_vv'].squeeze()), cmap='Greys_r', interpolation='none') \n",
    "            ax[1, 0].set_title('S1 snow-on vv')\n",
    "            ax[1, 1].imshow(np.log(data_dict['snowon_vh'].squeeze()), cmap='Greys_r', interpolation='none') \n",
    "            ax[1, 1].set_title('S1 snow-on vh')\n",
    "            ax[1, 2].imshow(np.log(data_dict['snowoff_vv'].squeeze()), cmap='Greys_r', interpolation='none') \n",
    "            ax[1, 2].set_title('S1 snow-off vv')\n",
    "            ax[1, 3].imshow(np.log(data_dict['snowoff_vh'].squeeze()), cmap='Greys_r', interpolation='none') \n",
    "            ax[1, 3].set_title('S1 snow-off vh')\n",
    "            ax[2, 0].imshow(data_dict['snowon_cr'].squeeze(), cmap='Purples', interpolation='none') \n",
    "            ax[2, 0].set_title('snow-on cross ratio')\n",
    "            ax[2, 1].imshow(data_dict['snowoff_cr'].squeeze(), cmap='Purples', interpolation='none') \n",
    "            ax[2, 1].set_title('snow-off cross ratio')\n",
    "            ax[2, 2].imshow(data_dict['delta_cr'].squeeze(), cmap='RdBu', interpolation='none') \n",
    "            ax[2, 2].set_title('delta cross ratio')\n",
    "            ax[2, 3].imshow(aso_gap_map.squeeze()+rtc_gap_map.squeeze(), cmap='Purples', interpolation='none') \n",
    "            ax[2, 3].set_title('aso and rtc gaps')\n",
    "            ax[0, 0].set_aspect('equal')\n",
    "            \n",
    "            # f.tight_layout()\n",
    "    else: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155a17a9-34e3-4c5a-b5ad-30726f910a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:crunchy-snow] *",
   "language": "python",
   "name": "conda-env-crunchy-snow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
