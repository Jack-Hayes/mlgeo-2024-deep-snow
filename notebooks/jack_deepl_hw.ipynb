{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "import rasterio as rio\n",
    "import rioxarray\n",
    "import math\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import deep_snow.models\n",
    "from deep_snow.dataset import norm_dict\n",
    "from deep_snow.utils import calc_norm, undo_norm, calc_dowy\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Preparation and Exploration (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AI-Ready Data Utilization (4%): Demonstrates the use of the previously prepared AI-ready dataset effectively, ensuring consistency in preprocessing across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3540"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see the directory below contains our ai-ready data\n",
    "files = glob(\"/mnt/c/Users/Jacke/uw/courses/aut24/ml_geo/jack_subsets/ncs/*.nc\")\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "html[data-theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: inline-block;\n",
       "  opacity: 0;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:focus + label {\n",
       "  border: 2px solid var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt; Size: 1MB\n",
       "Dimensions:    (x: 128, y: 128, samples: 16384)\n",
       "Dimensions without coordinates: x, y, samples\n",
       "Data variables: (12/14)\n",
       "    aso_sd     (x, y) float32 66kB ...\n",
       "    fcf        (x, y) float64 131kB ...\n",
       "    elevation  (x, y) float32 66kB ...\n",
       "    slope      (x, y) float64 131kB ...\n",
       "    tri        (x, y) float64 131kB ...\n",
       "    tpi        (x, y) float64 131kB ...\n",
       "    ...         ...\n",
       "    s1_pc1     (samples) float32 66kB ...\n",
       "    s1_pc2     (samples) float32 66kB ...\n",
       "    s2_pc1     (samples) float32 66kB ...\n",
       "    s2_pc2     (samples) float32 66kB ...\n",
       "    s2_pc3     (samples) float32 66kB ...\n",
       "    dowy       (x, y) int64 131kB ...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-19627c35-ca33-4bcc-b283-84ef1676574f' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-19627c35-ca33-4bcc-b283-84ef1676574f' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span>x</span>: 128</li><li><span>y</span>: 128</li><li><span>samples</span>: 16384</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-b8367dab-945e-4b95-8b7b-88c9460bc171' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b8367dab-945e-4b95-8b7b-88c9460bc171' class='xr-section-summary'  title='Expand/collapse section'>Coordinates: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-badc1239-82aa-4f95-bdb2-bef6bcd55b6c' class='xr-section-summary-in' type='checkbox'  checked><label for='section-badc1239-82aa-4f95-bdb2-bef6bcd55b6c' class='xr-section-summary' >Data variables: <span>(14)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>aso_sd</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0f1fe9f8-ed29-45a4-a434-dc0c5c4ee256' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-0f1fe9f8-ed29-45a4-a434-dc0c5c4ee256' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-246abb0b-d7e4-4927-ab64-0e17630bba30' class='xr-var-data-in' type='checkbox'><label for='data-246abb0b-d7e4-4927-ab64-0e17630bba30' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>fcf</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-69d14615-6502-43b3-bad0-1e7fa6048a6d' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-69d14615-6502-43b3-bad0-1e7fa6048a6d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-2ae94219-5459-4059-9d3f-9391201449ad' class='xr-var-data-in' type='checkbox'><label for='data-2ae94219-5459-4059-9d3f-9391201449ad' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>elevation</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-d9eba0b1-669e-497f-86e1-099ef5b9f8eb' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-d9eba0b1-669e-497f-86e1-099ef5b9f8eb' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8859b72e-54b8-4232-a2d3-0105ad7ab51e' class='xr-var-data-in' type='checkbox'><label for='data-8859b72e-54b8-4232-a2d3-0105ad7ab51e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>slope</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-c6331d29-57d5-4601-a213-b6054174f97d' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c6331d29-57d5-4601-a213-b6054174f97d' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-08918be3-5e2b-4150-9f43-59584e66ec8f' class='xr-var-data-in' type='checkbox'><label for='data-08918be3-5e2b-4150-9f43-59584e66ec8f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>tri</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-833a8ba0-1564-45db-8209-6b663824022b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-833a8ba0-1564-45db-8209-6b663824022b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-22df7e3c-42d4-41b5-91a9-16ce7db82df6' class='xr-var-data-in' type='checkbox'><label for='data-22df7e3c-42d4-41b5-91a9-16ce7db82df6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>tpi</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-9af6e47c-8fea-40c3-9779-3b4b196f76d3' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-9af6e47c-8fea-40c3-9779-3b4b196f76d3' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b4ce6d28-8a18-459e-b02e-21ecd54c6a12' class='xr-var-data-in' type='checkbox'><label for='data-b4ce6d28-8a18-459e-b02e-21ecd54c6a12' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>latitude</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-4ac6af72-e7fe-429a-a7f2-0c67064931e6' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-4ac6af72-e7fe-429a-a7f2-0c67064931e6' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4b7e188b-feae-4eb9-8774-cc63c3e9e81b' class='xr-var-data-in' type='checkbox'><label for='data-4b7e188b-feae-4eb9-8774-cc63c3e9e81b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>longitude</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-51da0c1c-a677-4443-873d-c6f7d81ba800' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-51da0c1c-a677-4443-873d-c6f7d81ba800' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-01bf6b2c-f7c7-4eec-9099-90d57c345a8e' class='xr-var-data-in' type='checkbox'><label for='data-01bf6b2c-f7c7-4eec-9099-90d57c345a8e' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float64]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>s1_pc1</span></div><div class='xr-var-dims'>(samples)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-2b6401bb-c257-4150-b60e-e6601f7a2ec1' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-2b6401bb-c257-4150-b60e-e6601f7a2ec1' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-186a88a1-b585-49f9-9995-d6ed1452ccdd' class='xr-var-data-in' type='checkbox'><label for='data-186a88a1-b585-49f9-9995-d6ed1452ccdd' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>s1_pc2</span></div><div class='xr-var-dims'>(samples)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-b7bcea5e-3923-4ee3-8977-2a09f44dea32' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-b7bcea5e-3923-4ee3-8977-2a09f44dea32' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-57e79006-0034-435f-a81e-1e9f4684a69a' class='xr-var-data-in' type='checkbox'><label for='data-57e79006-0034-435f-a81e-1e9f4684a69a' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>s2_pc1</span></div><div class='xr-var-dims'>(samples)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-60dbd16e-3e93-4e52-805e-a1ecfb872b12' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-60dbd16e-3e93-4e52-805e-a1ecfb872b12' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-5c84f6e1-b34f-416c-98ac-38920b3b7a76' class='xr-var-data-in' type='checkbox'><label for='data-5c84f6e1-b34f-416c-98ac-38920b3b7a76' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>s2_pc2</span></div><div class='xr-var-dims'>(samples)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-29ce80a6-e44b-4bf2-9940-bbf43f12ed21' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-29ce80a6-e44b-4bf2-9940-bbf43f12ed21' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6fba8592-5e7f-41bc-b378-662a99300357' class='xr-var-data-in' type='checkbox'><label for='data-6fba8592-5e7f-41bc-b378-662a99300357' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>s2_pc3</span></div><div class='xr-var-dims'>(samples)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-bafea6f8-5df7-4b7a-af72-660e3b2e2193' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-bafea6f8-5df7-4b7a-af72-660e3b2e2193' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9e72ce7a-7c53-424d-8a16-a385e7f1a730' class='xr-var-data-in' type='checkbox'><label for='data-9e72ce7a-7c53-424d-8a16-a385e7f1a730' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=float32]</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span>dowy</span></div><div class='xr-var-dims'>(x, y)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-a59d1400-608d-467e-9df1-aa49657f5bd0' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-a59d1400-608d-467e-9df1-aa49657f5bd0' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-277be2ba-02bd-4b22-8975-7a89a564222b' class='xr-var-data-in' type='checkbox'><label for='data-277be2ba-02bd-4b22-8975-7a89a564222b' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>[16384 values with dtype=int64]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-7f44e92e-332d-47aa-b8e2-d51e5003ab29' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-7f44e92e-332d-47aa-b8e2-d51e5003ab29' class='xr-section-summary'  title='Expand/collapse section'>Indexes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'></ul></div></li><li class='xr-section-item'><input id='section-e01712cb-e4a5-4878-a8f2-abd3d92eecd6' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-e01712cb-e4a5-4878-a8f2-abd3d92eecd6' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset> Size: 1MB\n",
       "Dimensions:    (x: 128, y: 128, samples: 16384)\n",
       "Dimensions without coordinates: x, y, samples\n",
       "Data variables: (12/14)\n",
       "    aso_sd     (x, y) float32 66kB ...\n",
       "    fcf        (x, y) float64 131kB ...\n",
       "    elevation  (x, y) float32 66kB ...\n",
       "    slope      (x, y) float64 131kB ...\n",
       "    tri        (x, y) float64 131kB ...\n",
       "    tpi        (x, y) float64 131kB ...\n",
       "    ...         ...\n",
       "    s1_pc1     (samples) float32 66kB ...\n",
       "    s1_pc2     (samples) float32 66kB ...\n",
       "    s2_pc1     (samples) float32 66kB ...\n",
       "    s2_pc2     (samples) float32 66kB ...\n",
       "    s2_pc3     (samples) float32 66kB ...\n",
       "    dowy       (x, y) int64 131kB ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now, let's open one of these files and understand the variables, shapes, and types of our data\n",
    "ds = xr.open_dataset(files[0])\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see above that all of our files are 128x128 rasters (50m spatial resolution) where each cell has 14 variables\n",
    "# note that the data here (besides the PCAs) are unscaled, the scaling happens in the data pipeline into the CNN \n",
    "# architectures as seen in section 3\n",
    "# the data is below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Target\n",
    "    - aso_sd (snow depth measured from aerial LiDAR)\n",
    "2. Spatial Attributes\n",
    "    - latitude (lat of respective 50x50m cell)\n",
    "    - longitude (lon of respective 50x50m cell)\n",
    "3. Temporal Attribute\n",
    "    - dowy (day of water year when LiDAR was collected)\n",
    "3. Topography Data\n",
    "    - elevation (m above egm2008 geoid)\n",
    "    - slope (degrees)\n",
    "    - tpi (topographic position index)\n",
    "    - tri (topographic roughness index)    \n",
    "4. Radar-derived Data\n",
    "    - s1_pc1 (Principal component 1 of PCA of Sentinel-1 backscatter)\n",
    "    - s1_pc2 (Principal component 2 of PCA of Sentinel-1 backscatter)\n",
    "5. Optical-imagery-derived data\n",
    "    - s2_pc1 (Principal component 1 of PCA of Sentinel-2 bands)\n",
    "    - s2_pc2 (Principal component 2 of PCA of Sentinel-2 bands)\n",
    "    - s2_pc3 (Principal component 3 of PCA of Sentinel-2 bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploratory Data Analysis (EDA) (3%): Includes visualizations and summaries to understand data distribution, temporal/spatial features, or domain-specific nuances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the nature of remote sensing data and pipelines to CNNs it's harder to visualized our AI-data this way\n",
    "# so we'll just plot some stats from our raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Setup (3%): Clearly defines the problem (e.g., regression/classification) and aligns the data with deep learning requirements (e.g., reshaping for CNNs, sequence creation for RNNs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Benchmarking Against CML (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Models (5%): Reports results from previous classical machine learning benchmarks (e.g., random forests, SVMs, or gradient boosting) with minimal additional work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Comparison (5%): Provides a high-level comparison of CML methods to deep learning models using relevant metrics (e.g., accuracy, RMSE, F1-score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Architecture Exploration (35%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation and Justification (8%): Implements at least three deep learning architectures (e.g., FCN, CNN, RNN, U-Net). Justifies architecture choice based on dataset and problem type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, let's just create a basic data pipeline to feed into a standard CNN\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from deep_snow.dataset import norm_dict\n",
    "from deep_snow.utils import calc_norm, undo_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_dict = {'aso_sd':[0, 25],\n",
    "             'vv':[-59, 30],\n",
    "             'vh':[-65, 17],\n",
    "             'cr':[-43, 16],\n",
    "             'delta_cr':[-33, 27],\n",
    "             'AOT':[0, 572],\n",
    "             'coastal':[0, 24304],\n",
    "             'blue':[0, 23371],\n",
    "             'green':[0, 26440],\n",
    "             'red':[0, 21576],\n",
    "             'red_edge1':[0, 20796],\n",
    "             'red_edge2':[0, 20432],\n",
    "             'red_edge3':[0, 20149],\n",
    "             'nir':[0, 21217],\n",
    "             'water_vapor':[0, 18199],\n",
    "             'swir1':[0, 17669],\n",
    "             'swir2':[0, 17936],\n",
    "             'scene_class_map':[0, 15],\n",
    "             'water_vapor_product':[0, 6518],\n",
    "             'elevation':[-100, 9000],\n",
    "             'aspect':[0, 360],\n",
    "             'slope':[0, 90],\n",
    "             'curvature':[-22, 22],\n",
    "             'tpi':[-164, 167],\n",
    "             'tri':[0, 913],\n",
    "             'latitude':[-90, 90],\n",
    "             'longitude':[-180, 180],\n",
    "             'dowy': [0, 365]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each file and normalize the relevant features\n",
    "def process_file(file_path):\n",
    "    ds = xr.open_dataset(file_path)\n",
    "    # Normalize features using the norm_dict\n",
    "    data_dict = {}\n",
    "    data_dict['latitude'] = calc_norm(torch.Tensor(ds['latitude'].values), norm_dict['latitude'])\n",
    "    data_dict['longitude'] = calc_norm(torch.Tensor(ds['longitude'].values), norm_dict['longitude'])\n",
    "    data_dict['elevation'] = calc_norm(torch.Tensor(ds['elevation'].values), norm_dict['elevation'])\n",
    "    data_dict['slope'] = calc_norm(torch.Tensor(ds['slope'].values), norm_dict['slope'])\n",
    "    data_dict['tri'] = calc_norm(torch.Tensor(ds['tri'].values), norm_dict['tri'])\n",
    "    data_dict['tpi'] = calc_norm(torch.Tensor(ds['tpi'].values), norm_dict['tpi'])\n",
    "    data_dict['dowy'] = calc_norm(torch.Tensor(ds['dowy'].values), norm_dict['dowy'])\n",
    "    # Reshape PC components to 2D\n",
    "    s1_pc1_2d = ds['s1_pc1'].values.reshape(128, 128)\n",
    "    s1_pc2_2d = ds['s1_pc2'].values.reshape(128, 128)\n",
    "    s2_pc1_2d = ds['s2_pc1'].values.reshape(128, 128)\n",
    "    s2_pc2_2d = ds['s2_pc2'].values.reshape(128, 128)\n",
    "    s2_pc3_2d = ds['s2_pc3'].values.reshape(128, 128)\n",
    "    # Stack normalized features\n",
    "    features = np.stack([\n",
    "        data_dict['elevation'].numpy(),\n",
    "        data_dict['slope'].numpy(),\n",
    "        data_dict['tri'].numpy(),\n",
    "        data_dict['tpi'].numpy(),\n",
    "        data_dict['latitude'].numpy(),\n",
    "        data_dict['longitude'].numpy(),\n",
    "        s1_pc1_2d,\n",
    "        s1_pc2_2d,\n",
    "        s2_pc1_2d,\n",
    "        s2_pc2_2d,\n",
    "        s2_pc3_2d,\n",
    "        data_dict['dowy'].numpy()\n",
    "    ], axis=0)\n",
    "    target = ds['aso_sd'].values\n",
    "    return features, target\n",
    "\n",
    "# Create datasets and data loaders\n",
    "def create_dataset(file_list):\n",
    "    features_list = []\n",
    "    targets_list = []\n",
    "    for file in file_list:\n",
    "        features, target = process_file(file)\n",
    "        features_list.append(torch.FloatTensor(features))\n",
    "        targets_list.append(torch.FloatTensor(target))\n",
    "    features_tensor = torch.stack(features_list)\n",
    "    targets_tensor = torch.stack(targets_list)\n",
    "    return TensorDataset(features_tensor, targets_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Split files into train, test, and validation sets\n",
    "files_subset = files[:1280]\n",
    "random.shuffle(files_subset)\n",
    "# Calculate the sizes for each split\n",
    "train_size = int(0.7 * len(files_subset))  # 70% for training\n",
    "test_size = int(0.2 * len(files_subset))   # 20% for testing\n",
    "val_size = len(files_subset) - train_size - test_size  # Remaining 10% for validation\n",
    "# Create the splits\n",
    "train_files = files_subset[:train_size]\n",
    "test_files = files_subset[train_size:train_size + test_size]\n",
    "val_files = files_subset[train_size + test_size:]\n",
    "# Create datasets\n",
    "train_dataset = create_dataset(train_files)\n",
    "test_dataset = create_dataset(test_files)\n",
    "val_dataset = create_dataset(val_files)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch shapes:\n",
      "Features: torch.Size([32, 12, 128, 128])\n",
      "Targets: torch.Size([32, 128, 128])\n",
      "\n",
      "Test batch shapes:\n",
      "Features: torch.Size([32, 12, 128, 128])\n",
      "Targets: torch.Size([32, 128, 128])\n",
      "\n",
      "Validation batch shapes:\n",
      "Features: torch.Size([32, 12, 128, 128])\n",
      "Targets: torch.Size([32, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Print shapes for training loader\n",
    "for features, targets in train_loader:\n",
    "    print(\"Training batch shapes:\")\n",
    "    print(f\"Features: {features.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break\n",
    "# Print shapes for test loader\n",
    "for features, targets in test_loader:\n",
    "    print(\"\\nTest batch shapes:\")\n",
    "    print(f\"Features: {features.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break\n",
    "# Print shapes for validation loader\n",
    "for features, targets in val_loader:\n",
    "    print(\"\\nValidation batch shapes:\")\n",
    "    print(f\"Features: {features.shape}\")\n",
    "    print(f\"Targets: {targets.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowDepthCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SnowDepthCNN, self).__init__()\n",
    "        # First conv layer: (12, 128, 128) -> (32, 128, 128)\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Second conv layer: (32, 128, 128) -> (16, 128, 128)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=16, kernel_size=3, padding=1)\n",
    "        # Final conv layer: (16, 128, 128) -> (1, 128, 128)\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x.squeeze(1)  # Remove channel dimension to match target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"\n",
    "    Train, validate, and test the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train and evaluate.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (Loss): The loss function.\n",
    "        optimizer (Optimizer): The optimizer for model training.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device to use for computation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing training losses, validation losses, and final test loss.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}'):\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "        \n",
    "        # Compute average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "    final_test_loss = test_loss / len(test_loader)\n",
    "    print(f'\\nFinal Test Loss: {final_test_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, final_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 28/28 [00:05<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 0.7114\n",
      "Validation Loss: 0.8744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 28/28 [00:02<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Training Loss: 0.5730\n",
      "Validation Loss: 0.4092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 28/28 [00:05<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Training Loss: 0.4115\n",
      "Validation Loss: 0.3952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 28/28 [00:05<00:00,  4.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Training Loss: 0.3903\n",
      "Validation Loss: 0.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 28/28 [00:05<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Training Loss: 0.3763\n",
      "Validation Loss: 0.3279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 28/28 [00:05<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Training Loss: 0.3364\n",
      "Validation Loss: 0.3187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 28/28 [00:03<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Training Loss: 0.3341\n",
      "Validation Loss: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 28/28 [00:05<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Training Loss: 0.3326\n",
      "Validation Loss: 0.3159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 28/28 [00:05<00:00,  4.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Training Loss: 0.3308\n",
      "Validation Loss: 0.3131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 28/28 [00:06<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Training Loss: 0.3288\n",
      "Validation Loss: 0.3118\n",
      "\n",
      "Final Test Loss: 0.3423\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = SnowDepthCNN()\n",
    "# Loss function: MAE is appropriate for continuous regression problems like snow depth\n",
    "criterion = nn.L1Loss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "# Call the function\n",
    "train_losses, val_losses, test_loss = train_and_evaluate_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# great, we have a basic CNN implemented\n",
    "# now let's explore other architectures for a preliminary search for the best architecture for our problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=12, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        # Encoding path\n",
    "        self.enc1 = self.contract_block(in_channels, 64, 3)\n",
    "        self.enc2 = self.contract_block(64, 128, 3)\n",
    "        # Bottleneck\n",
    "        self.bottleneck = self.contract_block(128, 256, 3)\n",
    "        # Decoding path\n",
    "        self.dec2 = self.expand_block(256, 128, 3)\n",
    "        self.dec1 = self.expand_block(128, 64, 3)\n",
    "        # Final output layer\n",
    "        self.final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    def contract_block(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "    def expand_block(self, in_channels, out_channels, kernel_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(out_channels, out_channels, kernel_size=2, stride=2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        bottleneck = self.bottleneck(enc2)\n",
    "        dec2 = self.dec2(bottleneck)\n",
    "        dec1 = self.dec1(dec2)\n",
    "        output = self.final(dec1)\n",
    "        # Resize the output to match the input size\n",
    "        return F.interpolate(output, size=(x.size(2), x.size(3)), mode='bilinear', align_corners=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jehayes/mambaforge/envs/deep-snow/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([4, 128, 128])) that is different to the input size (torch.Size([4, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1: 100%|██████████| 175/175 [01:00<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 0.3626\n",
      "Validation Loss: 0.3293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 175/175 [00:59<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Training Loss: 0.3560\n",
      "Validation Loss: 0.3282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 175/175 [01:00<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Training Loss: 0.3561\n",
      "Validation Loss: 0.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 175/175 [00:58<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Training Loss: 0.3561\n",
      "Validation Loss: 0.3303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 175/175 [01:04<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Training Loss: 0.3565\n",
      "Validation Loss: 0.3280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 175/175 [01:05<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Training Loss: 0.3560\n",
      "Validation Loss: 0.3281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 175/175 [01:05<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Training Loss: 0.3558\n",
      "Validation Loss: 0.3281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 175/175 [01:02<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Training Loss: 0.3561\n",
      "Validation Loss: 0.3304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 175/175 [01:04<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Training Loss: 0.3559\n",
      "Validation Loss: 0.3284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 175/175 [00:59<00:00,  2.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Training Loss: 0.3556\n",
      "Validation Loss: 0.3281\n",
      "\n",
      "Final Test Loss: 0.3663\n"
     ]
    }
   ],
   "source": [
    "model = UNet().to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, test_loss = train_and_evaluate_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hmm doesn't seem to be doing particularly well\n",
    "# granted this is a prelim test and batch sizes, model hyprerparameters, etc are all arbitrary\n",
    "# and we're also using less than 1/3 of our total data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our third architecture to explore will be a resnet\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=1)\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels\n",
    "            else nn.Identity()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        shortcut = self.shortcut(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return self.relu(x + shortcut)\n",
    "class ResNetSnowDepth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNetSnowDepth, self).__init__()\n",
    "        # Initial convolution: (12, 128, 128) -> (32, 128, 128)\n",
    "        self.initial_conv = nn.Conv2d(12, 32, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Residual blocks\n",
    "        self.res1 = ResidualBlock(32, 64)  # (32, 128, 128) -> (64, 128, 128)\n",
    "        self.res2 = ResidualBlock(64, 128)  # (64, 128, 128) -> (128, 128, 128)\n",
    "        self.res3 = ResidualBlock(128, 64)  # (128, 128, 128) -> (64, 128, 128)\n",
    "        # Output layer: (64, 128, 128) -> (1, 128, 128)\n",
    "        self.output_conv = nn.Conv2d(64, 1, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.initial_conv(x))\n",
    "        x = self.res1(x)\n",
    "        x = self.res2(x)\n",
    "        x = self.res3(x)\n",
    "        x = self.output_conv(x)\n",
    "        return x.squeeze(1)  # Remove channel dimension to match target shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 175/175 [02:12<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 0.4977\n",
      "Validation Loss: 0.3482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 175/175 [02:10<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]\n",
      "Training Loss: 0.3936\n",
      "Validation Loss: 1.0543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 175/175 [02:13<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]\n",
      "Training Loss: 0.4191\n",
      "Validation Loss: 0.3363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 175/175 [02:14<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]\n",
      "Training Loss: 0.3506\n",
      "Validation Loss: 0.3155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 175/175 [02:12<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]\n",
      "Training Loss: 0.3433\n",
      "Validation Loss: 0.3002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 175/175 [02:13<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]\n",
      "Training Loss: 0.3352\n",
      "Validation Loss: 0.3010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 175/175 [02:14<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]\n",
      "Training Loss: 0.3348\n",
      "Validation Loss: 0.2965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 175/175 [02:10<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]\n",
      "Training Loss: 0.3618\n",
      "Validation Loss: 1.0612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 175/175 [02:13<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]\n",
      "Training Loss: 0.3908\n",
      "Validation Loss: 0.3164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 175/175 [02:13<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]\n",
      "Training Loss: 0.3425\n",
      "Validation Loss: 0.3267\n",
      "\n",
      "Final Test Loss: 0.3646\n"
     ]
    }
   ],
   "source": [
    "model = ResNetSnowDepth()\n",
    "model = model.to(device)\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "train_losses, val_losses, test_loss = train_and_evaluate_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    test_loader=test_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# above we see that all models perform similarly, but the simplest CNN ran in 53 seconds while the Unet ran in 11 minutes\n",
    "# and the resnet ran in 23, so we'll proceed with a simple CNN\n",
    "# this prelim analysis is not robust and limited by computational resources, but for the sake of the project we'll proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning (8%): Explores hyperparameters (e.g., learning rate, number of layers, filter sizes) and documents experiments systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's change our train and eval model function to get rid of print statements\n",
    "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs, device, verbose=True):\n",
    "    \"\"\"\n",
    "    Train, validate, and test the model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train and evaluate.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (Loss): The loss function.\n",
    "        optimizer (Optimizer): The optimizer for model training.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device to use for computation.\n",
    "        verbose (bool): Whether to print detailed progress for each epoch.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing training losses, validation losses, and final test loss.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "        \n",
    "        # Compute average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    # Testing\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            test_loss += criterion(outputs, targets).item()\n",
    "\n",
    "    final_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'\\nFinal Test Loss: {final_test_loss:.4f}')\n",
    "\n",
    "    return train_losses, val_losses, final_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate=0.001, num_layers=3, filter_size=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jehayes/mambaforge/envs/deep-snow/lib/python3.12/site-packages/torch/nn/modules/loss.py:128: UserWarning: Using a target size (torch.Size([4, 128, 128])) that is different to the input size (torch.Size([4, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.l1_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with learning rate=0.001, num_layers=3, filter_size=32\n",
      "\n",
      "Training with learning rate=0.001, num_layers=3, filter_size=64\n",
      "\n",
      "Training with learning rate=0.001, num_layers=4, filter_size=16\n",
      "\n",
      "Training with learning rate=0.001, num_layers=4, filter_size=32\n",
      "\n",
      "Training with learning rate=0.001, num_layers=4, filter_size=64\n",
      "\n",
      "Training with learning rate=0.001, num_layers=5, filter_size=16\n",
      "\n",
      "Training with learning rate=0.001, num_layers=5, filter_size=32\n",
      "\n",
      "Training with learning rate=0.001, num_layers=5, filter_size=64\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=3, filter_size=16\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=3, filter_size=32\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=3, filter_size=64\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=4, filter_size=16\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=4, filter_size=32\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=4, filter_size=64\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=5, filter_size=16\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=5, filter_size=32\n",
      "\n",
      "Training with learning rate=0.0005, num_layers=5, filter_size=64\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=3, filter_size=16\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=3, filter_size=32\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=3, filter_size=64\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=4, filter_size=16\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=4, filter_size=32\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=4, filter_size=64\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=5, filter_size=16\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=5, filter_size=32\n",
      "\n",
      "Training with learning rate=0.0001, num_layers=5, filter_size=64\n",
      "\n",
      "Hyperparameter Tuning Results:\n",
      "{'learning_rate': 0.001, 'num_layers': 3, 'filter_size': 16, 'train_loss': 0.3573338877835444, 'val_loss': 0.3305337695963681, 'test_loss': 0.368077624887228}\n",
      "{'learning_rate': 0.001, 'num_layers': 3, 'filter_size': 32, 'train_loss': 0.3552712608075568, 'val_loss': 0.3281083146203309, 'test_loss': 0.3662978700082749}\n",
      "{'learning_rate': 0.001, 'num_layers': 3, 'filter_size': 64, 'train_loss': 0.3553758115800364, 'val_loss': 0.3281000435817987, 'test_loss': 0.3662944805528969}\n",
      "{'learning_rate': 0.001, 'num_layers': 4, 'filter_size': 16, 'train_loss': 0.35541169089797353, 'val_loss': 0.328252247555647, 'test_loss': 0.3664340456202626}\n",
      "{'learning_rate': 0.001, 'num_layers': 4, 'filter_size': 32, 'train_loss': 0.35576284729543006, 'val_loss': 0.3288072412274778, 'test_loss': 0.36658375166356566}\n",
      "{'learning_rate': 0.001, 'num_layers': 4, 'filter_size': 64, 'train_loss': 0.3554304539984358, 'val_loss': 0.32837311752140524, 'test_loss': 0.3665690005104989}\n",
      "{'learning_rate': 0.001, 'num_layers': 5, 'filter_size': 16, 'train_loss': 0.35525493925969515, 'val_loss': 0.32805210164748133, 'test_loss': 0.3662405071407557}\n",
      "{'learning_rate': 0.001, 'num_layers': 5, 'filter_size': 32, 'train_loss': 0.3569548799629722, 'val_loss': 0.3303144830837846, 'test_loss': 0.36791500508785246}\n",
      "{'learning_rate': 0.001, 'num_layers': 5, 'filter_size': 64, 'train_loss': 0.3554738392760711, 'val_loss': 0.3282780196331441, 'test_loss': 0.36646913149394095}\n",
      "{'learning_rate': 0.0005, 'num_layers': 3, 'filter_size': 16, 'train_loss': 0.35525635464116934, 'val_loss': 0.3280947241792455, 'test_loss': 0.36628414266742765}\n",
      "{'learning_rate': 0.0005, 'num_layers': 3, 'filter_size': 32, 'train_loss': 0.36338711713041577, 'val_loss': 0.3409798507019877, 'test_loss': 0.37276888348162174}\n",
      "{'learning_rate': 0.0005, 'num_layers': 3, 'filter_size': 64, 'train_loss': 0.35537661978263974, 'val_loss': 0.32828660952392963, 'test_loss': 0.3664647633116692}\n",
      "{'learning_rate': 0.0005, 'num_layers': 4, 'filter_size': 16, 'train_loss': 0.3552225206632699, 'val_loss': 0.32801839128602295, 'test_loss': 0.3662056342326105}\n",
      "{'learning_rate': 0.0005, 'num_layers': 4, 'filter_size': 32, 'train_loss': 0.3646167244708964, 'val_loss': 0.3298268663883209, 'test_loss': 0.367600106485188}\n",
      "{'learning_rate': 0.0005, 'num_layers': 4, 'filter_size': 64, 'train_loss': 0.3560569453718407, 'val_loss': 0.3288614793680608, 'test_loss': 0.36691220866516233}\n",
      "{'learning_rate': 0.0005, 'num_layers': 5, 'filter_size': 16, 'train_loss': 0.3804051530680486, 'val_loss': 0.36144015967845916, 'test_loss': 0.3847367265820503}\n",
      "{'learning_rate': 0.0005, 'num_layers': 5, 'filter_size': 32, 'train_loss': 0.35529533467920765, 'val_loss': 0.32803266019094734, 'test_loss': 0.36622054286301137}\n",
      "{'learning_rate': 0.0005, 'num_layers': 5, 'filter_size': 64, 'train_loss': 0.3621900914928743, 'val_loss': 0.3434350684285164, 'test_loss': 0.37838433265686033}\n",
      "{'learning_rate': 0.0001, 'num_layers': 3, 'filter_size': 16, 'train_loss': 0.4190732425983463, 'val_loss': 0.4657836352288723, 'test_loss': 0.4253150776028633}\n",
      "{'learning_rate': 0.0001, 'num_layers': 3, 'filter_size': 32, 'train_loss': 0.367802880938564, 'val_loss': 0.345638727247715, 'test_loss': 0.37770839549601076}\n",
      "{'learning_rate': 0.0001, 'num_layers': 3, 'filter_size': 64, 'train_loss': 0.408137959752764, 'val_loss': 0.45818749722093344, 'test_loss': 0.417393829151988}\n",
      "{'learning_rate': 0.0001, 'num_layers': 4, 'filter_size': 16, 'train_loss': 0.3761107510221856, 'val_loss': 0.35585923984646795, 'test_loss': 0.3852023781836033}\n",
      "{'learning_rate': 0.0001, 'num_layers': 4, 'filter_size': 32, 'train_loss': 0.4115228799730539, 'val_loss': 0.36363234624266627, 'test_loss': 0.39605240859091284}\n",
      "{'learning_rate': 0.0001, 'num_layers': 4, 'filter_size': 64, 'train_loss': 0.367492536178657, 'val_loss': 0.344195965602994, 'test_loss': 0.37635019689798355}\n",
      "{'learning_rate': 0.0001, 'num_layers': 5, 'filter_size': 16, 'train_loss': 0.38097251108714514, 'val_loss': 0.35004136234521865, 'test_loss': 0.3875591251254082}\n",
      "{'learning_rate': 0.0001, 'num_layers': 5, 'filter_size': 32, 'train_loss': 0.3552081823275824, 'val_loss': 0.32803059874800966, 'test_loss': 0.36621260884217915}\n",
      "{'learning_rate': 0.0001, 'num_layers': 5, 'filter_size': 64, 'train_loss': 0.5000981988225665, 'val_loss': 0.4661774742603302, 'test_loss': 0.4980780178308487}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# Define the hyperparameter grid\n",
    "learning_rates = [0.001, 0.0005, 0.0001]\n",
    "num_layers_options = [3, 4, 5]\n",
    "filter_sizes_options = [16, 32, 64]\n",
    "# Results storage\n",
    "tuning_results = []\n",
    "# Define a function to create the model with variable layers and filter sizes\n",
    "def create_model(num_layers, initial_filter_size):\n",
    "    layers = []\n",
    "    in_channels = 12\n",
    "    for i in range(num_layers):\n",
    "        out_channels = initial_filter_size if i == 0 else in_channels // 2\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        in_channels = out_channels\n",
    "    # Add the final layer\n",
    "    layers.append(nn.Conv2d(in_channels, 1, kernel_size=1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "# Train and evaluate models with different hyperparameters\n",
    "for lr, num_layers, filter_size in itertools.product(learning_rates, num_layers_options, filter_sizes_options):\n",
    "    print(f\"\\nTraining with learning rate={lr}, num_layers={num_layers}, filter_size={filter_size}\")\n",
    "    # Create the model\n",
    "    model = create_model(num_layers, filter_size)\n",
    "    model = model.to(device)\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Train the model\n",
    "    train_losses, val_losses, test_loss = train_and_evaluate_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        test_loader=test_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=num_epochs,\n",
    "        device=device,\n",
    "        verbose=False,  # Suppress progress for each epoch\n",
    "    )\n",
    "    # Record the results\n",
    "    tuning_results.append({\n",
    "        \"learning_rate\": lr,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"filter_size\": filter_size,\n",
    "        \"train_loss\": train_losses[-1],\n",
    "        \"val_loss\": val_losses[-1],\n",
    "        \"test_loss\": test_loss,\n",
    "    })\n",
    "# Display the tuning results\n",
    "print(\"\\nHyperparameter Tuning Results:\")\n",
    "for result in tuning_results:\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>filter_size</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.355223</td>\n",
       "      <td>0.328018</td>\n",
       "      <td>0.366206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.355208</td>\n",
       "      <td>0.328031</td>\n",
       "      <td>0.366213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.355295</td>\n",
       "      <td>0.328033</td>\n",
       "      <td>0.366221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.355255</td>\n",
       "      <td>0.328052</td>\n",
       "      <td>0.366241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.355256</td>\n",
       "      <td>0.328095</td>\n",
       "      <td>0.366284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.355376</td>\n",
       "      <td>0.328100</td>\n",
       "      <td>0.366294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.355271</td>\n",
       "      <td>0.328108</td>\n",
       "      <td>0.366298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.355412</td>\n",
       "      <td>0.328252</td>\n",
       "      <td>0.366434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.355377</td>\n",
       "      <td>0.328287</td>\n",
       "      <td>0.366465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.355474</td>\n",
       "      <td>0.328278</td>\n",
       "      <td>0.366469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.355430</td>\n",
       "      <td>0.328373</td>\n",
       "      <td>0.366569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.355763</td>\n",
       "      <td>0.328807</td>\n",
       "      <td>0.366584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.356057</td>\n",
       "      <td>0.328861</td>\n",
       "      <td>0.366912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.364617</td>\n",
       "      <td>0.329827</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>0.356955</td>\n",
       "      <td>0.330314</td>\n",
       "      <td>0.367915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.357334</td>\n",
       "      <td>0.330534</td>\n",
       "      <td>0.368078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.363387</td>\n",
       "      <td>0.340980</td>\n",
       "      <td>0.372769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>0.367493</td>\n",
       "      <td>0.344196</td>\n",
       "      <td>0.376350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0.367803</td>\n",
       "      <td>0.345639</td>\n",
       "      <td>0.377708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.362190</td>\n",
       "      <td>0.343435</td>\n",
       "      <td>0.378384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380405</td>\n",
       "      <td>0.361440</td>\n",
       "      <td>0.384737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0.376111</td>\n",
       "      <td>0.355859</td>\n",
       "      <td>0.385202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>0.380973</td>\n",
       "      <td>0.350041</td>\n",
       "      <td>0.387559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0.411523</td>\n",
       "      <td>0.363632</td>\n",
       "      <td>0.396052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>64</td>\n",
       "      <td>0.408138</td>\n",
       "      <td>0.458187</td>\n",
       "      <td>0.417394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0.419073</td>\n",
       "      <td>0.465784</td>\n",
       "      <td>0.425315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>5</td>\n",
       "      <td>64</td>\n",
       "      <td>0.500098</td>\n",
       "      <td>0.466177</td>\n",
       "      <td>0.498078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate  num_layers  filter_size  train_loss  val_loss  test_loss\n",
       "0          0.0005           4           16    0.355223  0.328018   0.366206\n",
       "1          0.0001           5           32    0.355208  0.328031   0.366213\n",
       "2          0.0005           5           32    0.355295  0.328033   0.366221\n",
       "3          0.0010           5           16    0.355255  0.328052   0.366241\n",
       "4          0.0005           3           16    0.355256  0.328095   0.366284\n",
       "5          0.0010           3           64    0.355376  0.328100   0.366294\n",
       "6          0.0010           3           32    0.355271  0.328108   0.366298\n",
       "7          0.0010           4           16    0.355412  0.328252   0.366434\n",
       "8          0.0005           3           64    0.355377  0.328287   0.366465\n",
       "9          0.0010           5           64    0.355474  0.328278   0.366469\n",
       "10         0.0010           4           64    0.355430  0.328373   0.366569\n",
       "11         0.0010           4           32    0.355763  0.328807   0.366584\n",
       "12         0.0005           4           64    0.356057  0.328861   0.366912\n",
       "13         0.0005           4           32    0.364617  0.329827   0.367600\n",
       "14         0.0010           5           32    0.356955  0.330314   0.367915\n",
       "15         0.0010           3           16    0.357334  0.330534   0.368078\n",
       "16         0.0005           3           32    0.363387  0.340980   0.372769\n",
       "17         0.0001           4           64    0.367493  0.344196   0.376350\n",
       "18         0.0001           3           32    0.367803  0.345639   0.377708\n",
       "19         0.0005           5           64    0.362190  0.343435   0.378384\n",
       "20         0.0005           5           16    0.380405  0.361440   0.384737\n",
       "21         0.0001           4           16    0.376111  0.355859   0.385202\n",
       "22         0.0001           5           16    0.380973  0.350041   0.387559\n",
       "23         0.0001           4           32    0.411523  0.363632   0.396052\n",
       "24         0.0001           3           64    0.408138  0.458187   0.417394\n",
       "25         0.0001           3           16    0.419073  0.465784   0.425315\n",
       "26         0.0001           5           64    0.500098  0.466177   0.498078"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_results_df = pd.DataFrame(tuning_results)\n",
    "tuning_results_df.sort_values(by='test_loss', ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we see above that we'll proceed with a learning rate of 0.0005, 4 layers, and a filter size of 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporation of Physics-Informed Loss (4%): Implements physics-informed loss where appropriate, with a clear explanation of its relevance to the geoscientific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicsInformedLoss(torch.nn.Module):\n",
    "    def __init__(self, lambda_gradient=0.1):\n",
    "        super(PhysicsInformedLoss, self).__init__()\n",
    "        self.lambda_gradient = lambda_gradient\n",
    "\n",
    "    def forward(self, predicted, target, features):\n",
    "        \"\"\"\n",
    "        Compute the physics-informed loss.\n",
    "\n",
    "        Args:\n",
    "            predicted (Tensor): Predicted snow depth of shape (batch_size, 1, H, W)\n",
    "            target (Tensor): Actual snow depth of shape (batch_size, H, W)\n",
    "            features (Tensor): Features tensor of shape (batch_size, 12, H, W) including TPI\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The computed loss value.\n",
    "        \"\"\"\n",
    "        # Traditional MAE loss (mean absolute error)\n",
    "        mae_loss = F.l1_loss(predicted, target)\n",
    "        # Extract TPI from the features tensor\n",
    "        tpi = features[:, 3, :, :]  # TPI is the 4th channel as seen from our data preprocessing\n",
    "        # Calculate gradients of predicted snow depth and target snow depth\n",
    "        grad_predicted = torch.abs(predicted[:, :, 1:, :] - predicted[:, :, :-1, :])  # Gradient in the x-direction\n",
    "        # If target is 3D (batch_size, height, width), add a channel dimension\n",
    "        target = target.unsqueeze(1)  # Adds a channel dimension (batch_size, 1, height, width)\n",
    "        # Then the gradient calculation can proceed with the added channel dimension:\n",
    "        grad_target = torch.abs(target[:, :, 1:, :] - target[:, :, :-1, :])  # Gradient in the x-direction\n",
    "        # Compute terrain gradient loss considering TPI and elevation\n",
    "        grad_tpi = torch.abs(tpi[:, 1:, :] - tpi[:, :-1, :])  # Gradient of TPI in x-direction\n",
    "        terrain_gradient_loss = torch.mean(torch.abs(grad_predicted - grad_target) * grad_tpi)  # Weighted by TPI gradient\n",
    "        # Total loss: MAE loss + weighted terrain gradient loss\n",
    "        total_loss = mae_loss + self.lambda_gradient * terrain_gradient_loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Physics-informed loss explanation (ChatGPT's, I need to research this on my own and understand it better and what lambda to use):\n",
    "\n",
    "Explanation of Physics-Informed Loss:\n",
    "\n",
    "\n",
    "Traditional MAE Loss:\n",
    "\n",
    "This term captures the basic prediction accuracy, comparing the predicted snow depth with the true snow depth at each pixel.\n",
    "\n",
    "\n",
    "Terrain Gradient Loss:\n",
    "\n",
    "We compute the gradient of both the predicted and actual snow depth with respect to neighboring pixels (in both the x and y directions).\n",
    "The terrain gradient loss ensures that the model doesn’t produce unrealistic sharp transitions in snow depth where there should be smooth changes, especially considering the relationship between snow depth and elevation. This helps to model the spatial continuity of snow depth, which should naturally change with elevation and other terrain features.\n",
    "\n",
    "\n",
    "Weighting Factor 𝜆\n",
    "\n",
    "The weighting factor λ controls the importance of the gradient loss relative to the traditional MAE loss. This allows you to balance between fitting the data and adhering to the physical constraints.\n",
    "\n",
    "\n",
    "Relevance to the Geoscientific Problem:\n",
    "\n",
    "Snow Accumulation and Melting: Snow depth predictions should adhere to physical laws such as the conservation of mass. Sharp, discontinuous changes in snow depth are not physically realistic, especially when they occur across similar elevations.\n",
    "\n",
    "\n",
    "Terrain Influence: \n",
    "Elevation plays a key role in snow accumulation and melting. The higher the elevation, the greater the snow accumulation. The physics-informed loss ensures that the snow depth predictions are consistent with the terrain features, making the model more physically grounded.\n",
    "\n",
    "\n",
    "Improved Generalization: \n",
    "By adding domain knowledge into the loss function, the model is guided to produce more physically realistic predictions. This can help prevent overfitting and lead to better generalization in real-world applications, where snow depth predictions are needed for tasks like avalanche forecasting, hydrological modeling, or climate studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SnowDepthCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SnowDepthCNN, self).__init__()\n",
    "        # Define the 4 layers with filter size 16\n",
    "        self.conv1 = nn.Conv2d(in_channels=12, out_channels=16, kernel_size=3, padding=1)  # 12 input channels (from features)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # 16 output channels\n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # 16 output channels\n",
    "        self.conv4 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1)  # 16 output channels\n",
    "        # Fully connected layer (input size should be 16 * 8 * 8 = 1024)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 512)  # Flattened output from conv layers: 16*8*8 = 1024\n",
    "        self.fc2 = nn.Linear(512, 128 * 128)  # Output size to match the target (128x128)\n",
    "    def forward(self, x):\n",
    "        # Pass input through convolutional layers with ReLU activations and max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool2d(x, 2)  # Max pooling\n",
    "        # Flatten the output for the fully connected layer\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor to (batch_size, 16 * 8 * 8 = 1024)\n",
    "        # Pass through fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        # Reshape back to (batch_size, 1, 128, 128) for snow depth prediction\n",
    "        x = x.view(-1, 1, 128, 128)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "def train_and_evaluate_model(model, train_loader, val_loader, test_loader, criterion, optimizer, num_epochs, device, verbose=True, save_dir='weights'):\n",
    "    \"\"\"\n",
    "    Train, validate, and test the model. Save model weights, logs, and metrics to JSON.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train and evaluate.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        test_loader (DataLoader): DataLoader for the test dataset.\n",
    "        criterion (Loss): The loss function.\n",
    "        optimizer (Optimizer): The optimizer for model training.\n",
    "        num_epochs (int): Number of training epochs.\n",
    "        device (torch.device): Device to use for computation.\n",
    "        verbose (bool): Whether to print detailed progress for each epoch.\n",
    "        save_dir (str): Directory to save model weights, logs, and metrics.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing training losses, validation losses, and final test loss.\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    performance_metrics = {}\n",
    "\n",
    "    # Make sure the save directory exists\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for features, targets in train_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, targets, features)  # Pass features as well\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for features, targets in val_loader:\n",
    "                features, targets = features.to(device), targets.to(device)\n",
    "                outputs = model(features)\n",
    "                val_loss += criterion(outputs, targets, features).item()  # Pass features as well\n",
    "        \n",
    "        # Compute average losses\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "            print(f'Training Loss: {avg_train_loss:.4f}')\n",
    "            print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        \n",
    "        # Save logs after each epoch (optional)\n",
    "        performance_metrics[f\"epoch_{epoch+1}\"] = {\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss\n",
    "        }\n",
    "\n",
    "    # Testing phase\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for features, targets in test_loader:\n",
    "            features, targets = features.to(device), targets.to(device)\n",
    "            outputs = model(features)\n",
    "            test_loss += criterion(outputs, targets, features).item()  # Pass features as well\n",
    "\n",
    "    final_test_loss = test_loss / len(test_loader)\n",
    "    performance_metrics[\"final_test_loss\"] = final_test_loss\n",
    "\n",
    "    if verbose:\n",
    "        print(f'\\nFinal Test Loss: {final_test_loss:.4f}')\n",
    "    \n",
    "    # Save model weights to file\n",
    "    model_weight_path = os.path.join(save_dir, 'jackk_pinn1.pth')\n",
    "    torch.save(model.state_dict(), model_weight_path)\n",
    "\n",
    "    # Save training logs and performance metrics to JSON file\n",
    "    performance_metrics_path = os.path.join(save_dir, 'performance_metrics.json')\n",
    "    with open(performance_metrics_path, 'w') as json_file:\n",
    "        json.dump(performance_metrics, json_file, indent=4)\n",
    "\n",
    "    return train_losses, val_losses, final_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5485/118312693.py:19: UserWarning: Using a target size (torch.Size([32, 128, 128])) that is different to the input size (torch.Size([32, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss = F.l1_loss(predicted, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 0.3760\n",
      "Validation Loss: 0.3191\n",
      "Epoch [2/10]\n",
      "Training Loss: 0.3338\n",
      "Validation Loss: 0.3138\n",
      "Epoch [3/10]\n",
      "Training Loss: 0.3308\n",
      "Validation Loss: 0.3129\n",
      "Epoch [4/10]\n",
      "Training Loss: 0.3303\n",
      "Validation Loss: 0.3126\n",
      "Epoch [5/10]\n",
      "Training Loss: 0.3305\n",
      "Validation Loss: 0.3125\n",
      "Epoch [6/10]\n",
      "Training Loss: 0.3301\n",
      "Validation Loss: 0.3124\n",
      "Epoch [7/10]\n",
      "Training Loss: 0.3298\n",
      "Validation Loss: 0.3123\n",
      "Epoch [8/10]\n",
      "Training Loss: 0.3298\n",
      "Validation Loss: 0.3123\n",
      "Epoch [9/10]\n",
      "Training Loss: 0.3298\n",
      "Validation Loss: 0.3123\n",
      "Epoch [10/10]\n",
      "Training Loss: 0.3298\n",
      "Validation Loss: 0.3123\n",
      "\n",
      "Final Test Loss: 0.3441\n"
     ]
    }
   ],
   "source": [
    "model = SnowDepthCNN()  # Your SnowDepthCNN model\n",
    "loss_fn = PhysicsInformedLoss(lambda_gradient=0.1)  # Physics-Informed Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adam optimizer\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Train and evaluate the model using your custom function\n",
    "train_losses, val_losses, test_loss = train_and_evaluate_model(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    criterion=loss_fn, optimizer=optimizer, \n",
    "    num_epochs=10, device=device, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_subset = files[:]\n",
    "random.shuffle(files_subset)\n",
    "train_size = int(0.7 * len(files_subset))  # 70% for training\n",
    "test_size = int(0.2 * len(files_subset))   # 20% for testing\n",
    "val_size = len(files_subset) - train_size - test_size  # Remaining 10% for validation\n",
    "train_files = files_subset[:train_size]\n",
    "test_files = files_subset[train_size:train_size + test_size]\n",
    "val_files = files_subset[train_size + test_size:]\n",
    "train_dataset = create_dataset(train_files)\n",
    "test_dataset = create_dataset(test_files)\n",
    "val_dataset = create_dataset(val_files)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5485/118312693.py:19: UserWarning: Using a target size (torch.Size([32, 128, 128])) that is different to the input size (torch.Size([32, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss = F.l1_loss(predicted, target)\n",
      "/tmp/ipykernel_5485/118312693.py:19: UserWarning: Using a target size (torch.Size([14, 128, 128])) that is different to the input size (torch.Size([14, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss = F.l1_loss(predicted, target)\n",
      "/tmp/ipykernel_5485/118312693.py:19: UserWarning: Using a target size (torch.Size([2, 128, 128])) that is different to the input size (torch.Size([2, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss = F.l1_loss(predicted, target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]\n",
      "Training Loss: 0.6477\n",
      "Validation Loss: 0.7031\n",
      "Epoch [2/10]\n",
      "Training Loss: 0.6397\n",
      "Validation Loss: 0.7024\n",
      "Epoch [3/10]\n",
      "Training Loss: 0.6364\n",
      "Validation Loss: 0.7021\n",
      "Epoch [4/10]\n",
      "Training Loss: 0.6397\n",
      "Validation Loss: 0.7020\n",
      "Epoch [5/10]\n",
      "Training Loss: 0.6434\n",
      "Validation Loss: 0.7020\n",
      "Epoch [6/10]\n",
      "Training Loss: 0.6375\n",
      "Validation Loss: 0.7020\n",
      "Epoch [7/10]\n",
      "Training Loss: 0.6404\n",
      "Validation Loss: 0.7020\n",
      "Epoch [8/10]\n",
      "Training Loss: 0.6352\n",
      "Validation Loss: 0.7021\n",
      "Epoch [9/10]\n",
      "Training Loss: 0.6404\n",
      "Validation Loss: 0.7020\n",
      "Epoch [10/10]\n",
      "Training Loss: 0.6398\n",
      "Validation Loss: 0.7021\n",
      "\n",
      "Final Test Loss: 0.5963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5485/118312693.py:19: UserWarning: Using a target size (torch.Size([4, 128, 128])) that is different to the input size (torch.Size([4, 1, 128, 128])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  mae_loss = F.l1_loss(predicted, target)\n"
     ]
    }
   ],
   "source": [
    "model = SnowDepthCNN()  # Your SnowDepthCNN model\n",
    "loss_fn = PhysicsInformedLoss(lambda_gradient=0.1)  # Physics-Informed Loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adam optimizer\n",
    "# Move the model to the appropriate device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# Train and evaluate the model using your custom function\n",
    "train_losses, val_losses, test_loss = train_and_evaluate_model(\n",
    "    model, train_loader, val_loader, test_loader, \n",
    "    criterion=loss_fn, optimizer=optimizer, \n",
    "    num_epochs=10, device=device, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as expected, the model performse worse when trained on the full dataset\n",
    "# this is expected due to the fact that the model was seeing very similar data in the first 1000 files\n",
    "# when we expanded it to the entire dataset, the model got exposed to data in mountainous regions with\n",
    "# different geological and physical properties, which it had not seen before\n",
    "# so creating a blanket model for all of these regions is more difficult"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-snow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
