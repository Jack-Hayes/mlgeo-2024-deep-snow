{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99a06752-1e1d-4445-a9f1-01585299962d",
   "metadata": {},
   "source": [
    "### Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f5800a9-2fb6-4d01-8866-dafaf113356e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import math\n",
    "import random\n",
    "import xarray as xr\n",
    "from torch.masked import masked_tensor, as_masked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cd13a8e-3d6b-45a1-b069-a61dd4db4176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/jovyan/shared-public/crunchy-snow/data/subsetsv1/train'\n",
    "path_list = glob(f'{data_dir}/ASO_50M_SD*.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f240fd28-1731-4c13-a867-6b1cffef51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are set by finding the min and max across the entire dataset\n",
    "norm_dict = {'aso_sd':[0, 24.9],\n",
    "             'vv':[0, 13523.8],\n",
    "             'vh':[0, 43.2],\n",
    "             'AOT':[0, 572.1],\n",
    "             'coastal':[0, 23459.1],\n",
    "             'blue':[0, 23004.1],\n",
    "             'green':[0, 26440.1],\n",
    "             'red':[0, 21576.1],\n",
    "             'red_edge1':[0, 20796.1],\n",
    "             'red_edge2':[0, 20432.1],\n",
    "             'red_edge3':[0, 20149.1],\n",
    "             'nir':[0, 21217.1],\n",
    "             'water_vapor':[0, 18199.1],\n",
    "             'swir1':[0, 17549.1],\n",
    "             'swir2':[0, 17314.1],\n",
    "             'scene_class_map':[0, 15],\n",
    "             'water_vapor_product':[0, 6517.5],\n",
    "             'elevation':[-100, 9000]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e367588-1e6f-4f72-953b-ec8b2b001bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_norm(tensor, minmax_list):\n",
    "    '''\n",
    "    normalize a tensor between 0 and 1 using a min and max value stored in a list\n",
    "    '''\n",
    "    normalized = (tensor-minmax_list[0])/(minmax_list[1]-minmax_list[0])\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca52124a-9fb5-404a-9de6-d5c90c457c2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define dataset \n",
    "class dataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    class that reads data from a netCDF and returns normalized tensors \n",
    "    '''\n",
    "    def __init__(self, path_list, norm_dict, norm=True):\n",
    "        self.path_list = path_list\n",
    "        self.norm_dict = norm_dict\n",
    "        self.norm = norm\n",
    "        \n",
    "    #dataset length\n",
    "    def __len__(self):\n",
    "        self.filelength = len(self.path_list)\n",
    "        return self.filelength\n",
    "    \n",
    "    #load images\n",
    "    def __getitem__(self,idx):\n",
    "        ds = xr.open_dataset(self.path_list[idx])\n",
    "        #ds = ds.coarsen(x = 6, boundary = 'trim').mean().coarsen(y = 6, boundary = 'trim').mean()\n",
    "        # convert to tensors\n",
    "        aso_sd = torch.from_numpy(ds.aso_sd.values)\n",
    "        snowon_vv = torch.from_numpy(ds.snowon_vv.values)\n",
    "        snowon_vh = torch.from_numpy(ds.snowon_vh.values)\n",
    "        snowoff_vv = torch.from_numpy(ds.snowoff_vv.values)\n",
    "        snowoff_vh = torch.from_numpy(ds.snowoff_vh.values)\n",
    "        snowon_vv_mean = torch.from_numpy(ds.snowon_vv_mean.values)\n",
    "        snowon_vh_mean = torch.from_numpy(ds.snowon_vh_mean.values)\n",
    "        snowoff_vv_mean = torch.from_numpy(ds.snowoff_vv_mean.values)\n",
    "        snowoff_vh_mean = torch.from_numpy(ds.snowoff_vh_mean.values)\n",
    "        aerosol_optical_thickness = torch.from_numpy(ds.AOT.values)\n",
    "        coastal_aerosol = torch.from_numpy(ds.B01.values)\n",
    "        blue = torch.from_numpy(ds.B02.values)\n",
    "        green = torch.from_numpy(ds.B03.values)\n",
    "        red = torch.from_numpy(ds.B04.values)\n",
    "        red_edge1 = torch.from_numpy(ds.B05.values)\n",
    "        red_edge2 = torch.from_numpy(ds.B06.values)\n",
    "        red_edge3 = torch.from_numpy(ds.B07.values)\n",
    "        nir = torch.from_numpy(ds.B08.values)\n",
    "        water_vapor = torch.from_numpy(ds.B09.values)\n",
    "        swir1 = torch.from_numpy(ds.B11.values)\n",
    "        swir2 = torch.from_numpy(ds.B12.values)\n",
    "        scene_class_map = torch.from_numpy(ds.SCL.values)\n",
    "        water_vapor_product = torch.from_numpy(ds.WVP.values)\n",
    "        fcf = torch.from_numpy(ds.fcf.values)\n",
    "        elevation = torch.from_numpy(ds.elevation.values)\n",
    "        aso_gap_map = torch.from_numpy(ds.aso_gap_map.values)\n",
    "        rtc_gap_map = torch.from_numpy(ds.rtc_gap_map.values)\n",
    "        rtc_mean_gap_map = torch.from_numpy(ds.rtc_mean_gap_map.values)\n",
    "        \n",
    "        # feature engineering / derived features\n",
    "        # NDSI\n",
    "        ndsi = green.subtract(swir1)/green.add(swir1)\n",
    "        # NDVI\n",
    "        ndvi = nir.subtract(red)/nir.add(red)\n",
    "        # vh/vv\n",
    "        # Heat Load Index\n",
    "        # slopes\n",
    "        \n",
    "        # normalize layers (except gap maps and fcf)\n",
    "        if self.norm == True:\n",
    "            aso_sd = calc_norm(aso_sd, self.norm_dict['aso_sd'])\n",
    "            snowon_vv = calc_norm(snowon_vv, self.norm_dict['vv'])\n",
    "            snowon_vh = calc_norm(snowon_vh, self.norm_dict['vh'])\n",
    "            snowoff_vv = calc_norm(snowoff_vv, self.norm_dict['vv'])\n",
    "            snowoff_vh = calc_norm(snowoff_vh, self.norm_dict['vh'])\n",
    "            snowon_vv_mean = calc_norm(snowon_vv_mean, self.norm_dict['vv'])\n",
    "            snowon_vh_mean = calc_norm(snowon_vh_mean, self.norm_dict['vh'])\n",
    "            snowoff_vv_mean = calc_norm(snowoff_vv_mean, self.norm_dict['vv'])\n",
    "            snowoff_vh_mean = calc_norm(snowoff_vh_mean, self.norm_dict['vh'])\n",
    "            aerosol_optical_thickness = calc_norm(aerosol_optical_thickness, self.norm_dict['AOT'])\n",
    "            coastal_aerosol = calc_norm(coastal_aerosol, self.norm_dict['coastal'])\n",
    "            blue = calc_norm(blue, self.norm_dict['blue'])\n",
    "            green = calc_norm(green, self.norm_dict['green'])\n",
    "            red = calc_norm(red, self.norm_dict['red'])\n",
    "            red_edge1 = calc_norm(red_edge1, self.norm_dict['red_edge1'])\n",
    "            red_edge2 = calc_norm(red_edge2, self.norm_dict['red_edge2'])\n",
    "            red_edge3 = calc_norm(red_edge3, self.norm_dict['red_edge3'])\n",
    "            nir = calc_norm(nir, self.norm_dict['nir'])\n",
    "            water_vapor = calc_norm(water_vapor, self.norm_dict['water_vapor'])\n",
    "            swir1 = calc_norm(swir1, self.norm_dict['swir1'])\n",
    "            swir2 = calc_norm(swir2, self.norm_dict['swir2'])\n",
    "            scene_class_map = calc_norm(scene_class_map, self.norm_dict['scene_class_map'])\n",
    "            water_vapor_product = calc_norm(water_vapor_product, self.norm_dict['water_vapor_product'])\n",
    "            elevation = calc_norm(elevation, self.norm_dict['elevation'])\n",
    "        \n",
    "        # return only selected bands, for now\n",
    "        # return green, swir1\n",
    "        # return ndsi, ndvi\n",
    "        return aso_sd, snowon_vv, snowon_vh, snowoff_vv, snowoff_vh, blue, green, red, nir, swir1, swir2, fcf, elevation, aso_gap_map, rtc_gap_map, ndsi, ndvi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3bfaa8c-d100-492f-908b-b7e8085b4238",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create dataloaders\n",
    "train_data = dataset(path_list, norm_dict, norm=True)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1128041-af59-4320-9b35-07dbf1e8bbc5",
   "metadata": {},
   "source": [
    "### Boilerplate CNN\n",
    "##### (hopefully with model architecture flexibility)\n",
    "Most of the following code was taken and adjusted from https://github.com/nikhilroxtomar/Semantic-Segmentation-Architecture/blob/main/PyTorch/unet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb12869-7708-4533-a872-17474d2dba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "055d682d-4912-45e7-bd64-9ef48a5307df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutional layer\n",
    "class cnn_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        # CNN + relu layer\n",
    "        x_conv1 = self.conv1(x_in)\n",
    "        x_relu1 = self.relu(x_conv1)\n",
    "        # CNN + relu layer\n",
    "        x_conv2 = self.conv2(x_relu1)\n",
    "        x_out = self.relu(x_conv2)\n",
    "        return x_out\n",
    "\n",
    "# cnn + max pooling block\n",
    "class conv_pool_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = cnn_block(in_channel, out_channel)\n",
    "        self.pool = nn.MaxPool2d((2, 2))\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x_out = cnn_block(in_channel, out_channel)\n",
    "        p_out = self.pool(x_out)\n",
    "\n",
    "        return x_out, p_out\n",
    "\n",
    "\n",
    "# upsample\n",
    "class inverse_conv_block(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample = nn.ConvTranspose2d(in_channel, out_channel, kernel_size=2, stride=2, padding=0)\n",
    "        self.conv = cnn_block(out_channel+out_channel, out_channel)\n",
    "\n",
    "    def forward(self, x_in, skip):\n",
    "        x_up = self.upsample(x_in)\n",
    "        x_cat = torch.cat([x_up, skip], axis=1)\n",
    "        x_out = self.conv(x_cat)\n",
    "        return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cf6a4c5-123f-4ec1-abad-924bf4f5096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the u-net\n",
    "class build_cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \"\"\" convolutions + pooling \"\"\"\n",
    "        self.cp1 = conv_pool_block(3, 32)\n",
    "        self.cp2 = conv_pool_block(32, 64)\n",
    "        self.cp3 = conv_pool_block(64, 128)\n",
    "        self.cp4 = conv_pool_block(128, 256)\n",
    "\n",
    "        \"\"\" convolution \"\"\"\n",
    "        self.b = cnn_block(256, 512)\n",
    "\n",
    "        \"\"\" inverse convolutions \"\"\"\n",
    "        self.ic1 = inverse_conv_block(512, 256)\n",
    "        self.ic2 = inverse_conv_block(256, 128)\n",
    "        self.ic3 = inverse_conv_block(128, 64)\n",
    "        self.ic4 = inverse_conv_block(64, 32)\n",
    "\n",
    "        \"\"\" output \"\"\"\n",
    "        self.outputs = nn.Conv2d(32, 1, kernel_size=1, padding=0)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" conv + pool \"\"\"\n",
    "        x1, p1 = self.cp1(inputs)\n",
    "        x2, p2 = self.cp2(p1)\n",
    "        x3, p3 = self.cp3(p2)\n",
    "        x4, p4 = self.cp4(p3)\n",
    "\n",
    "        \"\"\" conv \"\"\"\n",
    "        b = self.b(p4)\n",
    "\n",
    "        \"\"\" upsample \"\"\"\n",
    "        u1 = self.ic1(b, x4)\n",
    "        u2 = self.ic2(u1, x3)\n",
    "        u3 = self.ic3(u2, x2)\n",
    "        u4 = self.ic4(u3, x1)\n",
    "\n",
    "        \"\"\" Classifier \"\"\"\n",
    "        outputs = self.outputs(u4)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a87f1b5-5fb9-44c3-9d01-acd82cd84f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68732782-a6ec-4721-8e83-7ff67f59857c",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8543826-354e-41d2-8505-c686bb890ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
